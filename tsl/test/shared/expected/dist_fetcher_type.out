-- This file and its contents are licensed under the Timescale License.
-- Please see the included NOTICE for copyright information and
-- LICENSE-TIMESCALE for a copy of the license.
\set ON_ERROR_STOP off
-- Test that we use the correct type of remote data fetcher.
set timescaledb.remote_data_fetcher = 'auto';
select 1 x from distinct_on_distributed t1, distinct_on_distributed t2
where t1.id = t2.id + 1
limit 1;
 x 
 1
(1 row)

-- This query should choose row-by-row fetcher.
select 1 x from distinct_on_distributed t1
limit 1;
 x 
 1
(1 row)

explain (analyze, verbose, costs off, timing off, summary off)
select 1 x from distinct_on_distributed t1
limit 1;
QUERY PLAN
 Limit (actual rows=1 loops=1)
   Output: 1
   ->  Custom Scan (DataNodeScan) on public.distinct_on_distributed t1 (actual rows=1 loops=1)
         Output: 1
         Data node: data_node_1
         Fetcher Type: Row by row
         Chunks: _dist_hyper_X_X_chunk
         Remote SQL: SELECT NULL FROM public.distinct_on_distributed WHERE _timescaledb_internal.chunks_in(public.distinct_on_distributed.*, ARRAY[..]) LIMIT 1
(8 rows)

set timescaledb.remote_data_fetcher = 'cursor';
select 1 x from distinct_on_distributed t1, distinct_on_distributed t2
where t1.id = t2.id + 1
limit 1;
 x 
 1
(1 row)

explain (analyze, verbose, costs off, timing off, summary off)
select 1 x from distinct_on_distributed t1, distinct_on_distributed t2
where t1.id = t2.id + 1
limit 1;
QUERY PLAN
 Limit (actual rows=1 loops=1)
   Output: 1
   ->  Nested Loop (actual rows=1 loops=1)
         Output: 1
         Join Filter: (t1.id = (t2.id + 1))
         Rows Removed by Join Filter: 4
         ->  Custom Scan (DataNodeScan) on public.distinct_on_distributed t1 (actual rows=1 loops=1)
               Output: t1.id
               Data node: data_node_1
               Fetcher Type: Cursor
               Chunks: _dist_hyper_X_X_chunk
               Remote SQL: SELECT id FROM public.distinct_on_distributed WHERE _timescaledb_internal.chunks_in(public.distinct_on_distributed.*, ARRAY[..])
         ->  Materialize (actual rows=5 loops=1)
               Output: t2.id
               ->  Custom Scan (DataNodeScan) on public.distinct_on_distributed t2 (actual rows=5 loops=1)
                     Output: t2.id
                     Data node: data_node_1
                     Fetcher Type: Cursor
                     Chunks: _dist_hyper_X_X_chunk
                     Remote SQL: SELECT id FROM public.distinct_on_distributed WHERE _timescaledb_internal.chunks_in(public.distinct_on_distributed.*, ARRAY[..])
(20 rows)

-- This query can't work with rowbyrow fetcher.
set timescaledb.remote_data_fetcher = 'rowbyrow';
select 1 x from distinct_on_distributed t1, distinct_on_distributed t2
where t1.id = t2.id + 1
limit 1;
ERROR:  could not set single-row mode on connection to "data_node_1"
-- Check once again that 'auto' is used after 'rowbyrow'.
set timescaledb.remote_data_fetcher = 'auto';
select 1 x from distinct_on_distributed t1, distinct_on_distributed t2
where t1.id = t2.id + 1
limit 1;
 x 
 1
(1 row)

reset timescaledb.remote_data_fetcher;
-- #3786 test for assertion failure in cursor_fetcher_rewind
SET jit TO off;
SELECT *
FROM devices AS d
WHERE
  EXISTS(
    SELECT 1
    FROM metrics_dist AS m,
      LATERAL(
        SELECT 1
        FROM insert_test it
        WHERE
          EXISTS(
            SELECT 1
            FROM dist_chunk_copy AS ref_2
            WHERE
              it.id IS NOT NULL AND
              EXISTS(SELECT d.name AS c0 FROM metrics_int WHERE NULL::TIMESTAMP <= m.time)
          )
      ) AS l
    WHERE d.name ~~ d.name
  )
ORDER BY 1,2;
 device_id | name 
-----------+------
(0 rows)

RESET jit;
-- Row by row fetcher should fail on a custom type that has no binary
-- serialization.
set timescaledb.remote_data_fetcher = 'rowbyrow';
explain (analyze, verbose, costs off, timing off, summary off)
select time, txn_id, val, substring(info for 20) from disttable_with_ct;
ERROR:  cannot use row-by-row fetcher because some of the column types do not have binary serialization
-- Cursor fetcher should be chosen automatically if we have a data type with no
-- binary serialization.
set timescaledb.remote_data_fetcher = 'auto';
explain (analyze, verbose, costs off, timing off, summary off)
select * from disttable_with_ct;
QUERY PLAN
 Custom Scan (DataNodeScan) on public.disttable_with_ct (actual rows=2 loops=1)
   Output: disttable_with_ct."time", disttable_with_ct.txn_id, disttable_with_ct.val, disttable_with_ct.info
   Data node: data_node_2
   Fetcher Type: Cursor
   Chunks: _dist_hyper_X_X_chunk
   Remote SQL: SELECT "time", txn_id, val, info FROM public.disttable_with_ct WHERE _timescaledb_internal.chunks_in(public.disttable_with_ct.*, ARRAY[..])
(6 rows)

-- Row by row fetcher with bytea data
set timescaledb.remote_data_fetcher = 'rowbyrow';
explain (analyze, verbose, costs off, timing off, summary off)
select * from disttable_with_bytea;
QUERY PLAN
 Custom Scan (DataNodeScan) on public.disttable_with_bytea (actual rows=2 loops=1)
   Output: disttable_with_bytea."time", disttable_with_bytea.bdata
   Data node: data_node_3
   Fetcher Type: Row by row
   Chunks: _dist_hyper_X_X_chunk
   Remote SQL: SELECT "time", bdata FROM public.disttable_with_bytea WHERE _timescaledb_internal.chunks_in(public.disttable_with_bytea.*, ARRAY[..])
(6 rows)

select * from disttable_with_bytea;
 time | bdata 
------+-------
 1001 | \x
 1001 | 
(2 rows)

-- Cursor fetcher with bytea data
set timescaledb.remote_data_fetcher = 'cursor';
explain (analyze, verbose, costs off, timing off, summary off)
select * from disttable_with_bytea;
QUERY PLAN
 Custom Scan (DataNodeScan) on public.disttable_with_bytea (actual rows=2 loops=1)
   Output: disttable_with_bytea."time", disttable_with_bytea.bdata
   Data node: data_node_3
   Fetcher Type: Cursor
   Chunks: _dist_hyper_X_X_chunk
   Remote SQL: SELECT "time", bdata FROM public.disttable_with_bytea WHERE _timescaledb_internal.chunks_in(public.disttable_with_bytea.*, ARRAY[..])
(6 rows)

select * from disttable_with_bytea;
 time | bdata 
------+-------
 1001 | \x
 1001 | 
(2 rows)

