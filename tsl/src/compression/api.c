/*
 * This file and its contents are licensed under the Timescale License.
 * Please see the included NOTICE for copyright information and
 * LICENSE-TIMESCALE for a copy of the license.
 */

/* This file contains the implementation for SQL utility functions that
 *  compress and decompress chunks
 */
#include <postgres.h>
#include <access/valid.h>
#include <access/xact.h>
#include <catalog/dependency.h>
#include <commands/tablecmds.h>
#include <commands/trigger.h>
#include <miscadmin.h>
#include <nodes/makefuncs.h>
#include <nodes/pg_list.h>
#include <nodes/parsenodes.h>
#include <parser/parse_func.h>
#include <storage/lmgr.h>
#include <trigger.h>
#include <utils/builtins.h>
#include <utils/elog.h>
#include <utils/fmgrprotos.h>
#include <libpq-fe.h>
#include <utils/snapmgr.h>
#include <utils/syscache.h>
#include <utils/inval.h>

#include <remote/dist_commands.h>
#include "compat/compat.h"
#include "cache.h"
#include "chunk.h"
#include "debug_point.h"
#include "errors.h"
#include "error_utils.h"
#include "hypercube.h"
#include "hypertable.h"
#include "hypertable_cache.h"
#include "ts_catalog/continuous_agg.h"
#include "ts_catalog/hypertable_compression.h"
#include "ts_catalog/compression_chunk_size.h"
#include "create.h"
#include "api.h"
#include "compression.h"
#include "compat/compat.h"
#include "scanner.h"
#include "scan_iterator.h"
#include "utils.h"
#include "guc.h"

/* build scankeys for segmentby columns */
#define SEGMENTBY_KEYS (1 << 1)
/* build scankeys for orderby columns */
#define ORDERBY_KEYS (1 << 2)
/* build scankeys for _ts_meta_sequence_num columns */
#define SEQNUM_KEYS (1 << 3)
/* build scankeys for _ts_meta_sequence_num columns with next value */
#define NEXT_SEQNUM_KEYS (1 << 4)

typedef struct CompressTuplesortstateCxt
{
	Tuplesortstate *tuplestore;
	AttrNumber *sort_keys;
	Oid *sort_operators;
	Oid *sort_collations;
	bool *nulls_first;
} CompressTuplesortstateCxt;

/*
 * Placeholder to save all context related to compressed chunk
 */
typedef struct CompressedChunkStats
{
	/* total no: of uncompressed rows */
	unsigned long num_rows_pre_compression;
	/* total no: of compressed rows */
	unsigned long num_rows_post_compression;
	/* total count of gaps between compressed rows */
	unsigned long num_gaps_in_sequences;
	/* total count of gaps between compressed rows generated by compress_chunk() API */
	unsigned long num_expected_gaps_in_sequences;
	/* total unique segmentby column values */
	unsigned long num_segments;
	/* total number of compressed batches per unique segmentby column values */
	unsigned long num_batches_per_segments[MAX_ROWS_PER_COMPRESSION];
} CompressedChunkStats;

/*
 * Placeholder to save all context related to recompression of partial chunks
 */
typedef struct AffectedSegmentsCxt
{
	/* holds matching compressed tuples */
	HeapTuple *compressed_tuples;
	/* holds total number of uncompressed tuples which match the compressed tuple */
	unsigned long *num_matching_uncompressedrows;
	/* holds total affected compressed segments */
	unsigned long total_affected_compressed_rows;
	/* flag to know if new segments are found in uncompressed chunk */
	bool new_segments_found;
	/* holds total number of uncompressed tuples with same segmentby column values */
	unsigned long *num_non_matching_compressedrows;
	/* holds total affected new segmentby column values */
	unsigned long total_new_compressed_segments;
	/* default size of above arrays */
	unsigned long default_size;
	/* default size of new segments arrays */
    unsigned long new_segments_default_size;
} AffectedSegmentsCxt;

typedef struct CompressChunkCxt
{
	Hypertable *srcht;
	Chunk *srcht_chunk;		 /* chunk from srcht */
	Hypertable *compress_ht; /*compressed table for srcht */
} CompressChunkCxt;

static void recompress_partial_chunks(Chunk *uncompressed_chunk, Chunk *compressed_chunk);

static void
compression_chunk_size_catalog_insert(int32 src_chunk_id, const RelationSize *src_size,
									  int32 compress_chunk_id, const RelationSize *compress_size,
									  int64 rowcnt_pre_compression, int64 rowcnt_post_compression)
{
	Catalog *catalog = ts_catalog_get();
	Relation rel;
	TupleDesc desc;
	CatalogSecurityContext sec_ctx;

	Datum values[Natts_compression_chunk_size];
	bool nulls[Natts_compression_chunk_size] = { false };

	rel = table_open(catalog_get_table_id(catalog, COMPRESSION_CHUNK_SIZE), RowExclusiveLock);
	desc = RelationGetDescr(rel);

	memset(values, 0, sizeof(values));

	values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_chunk_id)] =
		Int32GetDatum(src_chunk_id);
	values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_compressed_chunk_id)] =
		Int32GetDatum(compress_chunk_id);
	values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_uncompressed_heap_size)] =
		Int64GetDatum(src_size->heap_size);
	values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_uncompressed_toast_size)] =
		Int64GetDatum(src_size->toast_size);
	values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_uncompressed_index_size)] =
		Int64GetDatum(src_size->index_size);
	values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_compressed_heap_size)] =
		Int64GetDatum(compress_size->heap_size);
	values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_compressed_toast_size)] =
		Int64GetDatum(compress_size->toast_size);
	values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_compressed_index_size)] =
		Int64GetDatum(compress_size->index_size);
	values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_numrows_pre_compression)] =
		Int64GetDatum(rowcnt_pre_compression);
	values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_numrows_post_compression)] =
		Int64GetDatum(rowcnt_post_compression);

	ts_catalog_database_info_become_owner(ts_catalog_database_info_get(), &sec_ctx);
	ts_catalog_insert_values(rel, desc, values, nulls);
	ts_catalog_restore_user(&sec_ctx);
	table_close(rel, RowExclusiveLock);
}

static int
compression_chunk_size_catalog_update_merged(int32 chunk_id, const RelationSize *size,
											 int32 merge_chunk_id, const RelationSize *merge_size,
											 int64 merge_rowcnt_pre_compression,
											 int64 merge_rowcnt_post_compression)
{
	ScanIterator iterator =
		ts_scan_iterator_create(COMPRESSION_CHUNK_SIZE, RowExclusiveLock, CurrentMemoryContext);
	bool updated = false;

	iterator.ctx.index =
		catalog_get_index(ts_catalog_get(), COMPRESSION_CHUNK_SIZE, COMPRESSION_CHUNK_SIZE_PKEY);
	ts_scan_iterator_scan_key_init(&iterator,
								   Anum_compression_chunk_size_pkey_chunk_id,
								   BTEqualStrategyNumber,
								   F_INT4EQ,
								   Int32GetDatum(chunk_id));
	ts_scanner_foreach(&iterator)
	{
		Datum values[Natts_compression_chunk_size];
		bool replIsnull[Natts_compression_chunk_size] = { false };
		bool repl[Natts_compression_chunk_size] = { false };
		bool should_free;
		TupleInfo *ti = ts_scan_iterator_tuple_info(&iterator);
		HeapTuple tuple = ts_scanner_fetch_heap_tuple(ti, false, &should_free);
		HeapTuple new_tuple;
		heap_deform_tuple(tuple, ts_scanner_get_tupledesc(ti), values, replIsnull);

		/* Increment existing sizes with sizes from uncompressed chunk. */
		values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_uncompressed_heap_size)] =
			Int64GetDatum(size->heap_size +
						  DatumGetInt64(values[AttrNumberGetAttrOffset(
							  Anum_compression_chunk_size_uncompressed_heap_size)]));
		repl[AttrNumberGetAttrOffset(Anum_compression_chunk_size_uncompressed_heap_size)] = true;
		values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_uncompressed_toast_size)] =
			Int64GetDatum(size->toast_size +
						  DatumGetInt64(values[AttrNumberGetAttrOffset(
							  Anum_compression_chunk_size_uncompressed_toast_size)]));
		repl[AttrNumberGetAttrOffset(Anum_compression_chunk_size_uncompressed_toast_size)] = true;
		values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_uncompressed_index_size)] =
			Int64GetDatum(size->index_size +
						  DatumGetInt64(values[AttrNumberGetAttrOffset(
							  Anum_compression_chunk_size_uncompressed_index_size)]));
		repl[AttrNumberGetAttrOffset(Anum_compression_chunk_size_uncompressed_index_size)] = true;
		values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_compressed_heap_size)] =
			Int64GetDatum(merge_size->heap_size);
		repl[AttrNumberGetAttrOffset(Anum_compression_chunk_size_compressed_heap_size)] = true;
		values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_compressed_toast_size)] =
			Int64GetDatum(merge_size->toast_size);
		repl[AttrNumberGetAttrOffset(Anum_compression_chunk_size_compressed_toast_size)] = true;
		values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_compressed_index_size)] =
			Int64GetDatum(merge_size->index_size);
		repl[AttrNumberGetAttrOffset(Anum_compression_chunk_size_compressed_index_size)] = true;
		values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_numrows_pre_compression)] =
			Int64GetDatum(merge_rowcnt_pre_compression +
						  DatumGetInt64(values[AttrNumberGetAttrOffset(
							  Anum_compression_chunk_size_numrows_pre_compression)]));
		repl[AttrNumberGetAttrOffset(Anum_compression_chunk_size_numrows_pre_compression)] = true;
		values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_numrows_post_compression)] =
			Int64GetDatum(merge_rowcnt_post_compression +
						  DatumGetInt64(values[AttrNumberGetAttrOffset(
							  Anum_compression_chunk_size_numrows_post_compression)]));
		repl[AttrNumberGetAttrOffset(Anum_compression_chunk_size_numrows_post_compression)] = true;

		new_tuple =
			heap_modify_tuple(tuple, ts_scanner_get_tupledesc(ti), values, replIsnull, repl);
		ts_catalog_update(ti->scanrel, new_tuple);
		heap_freetuple(new_tuple);

		if (should_free)
			heap_freetuple(tuple);

		updated = true;
		break;
	}

	ts_scan_iterator_end(&iterator);
	ts_scan_iterator_close(&iterator);
	return updated;
}

static void
get_hypertable_or_cagg_name(Hypertable *ht, Name objname)
{
	ContinuousAggHypertableStatus status = ts_continuous_agg_hypertable_status(ht->fd.id);
	if (status == HypertableIsNotContinuousAgg || status == HypertableIsRawTable)
		namestrcpy(objname, NameStr(ht->fd.table_name));
	else if (status == HypertableIsMaterialization)
	{
		ContinuousAgg *cagg = ts_continuous_agg_find_by_mat_hypertable_id(ht->fd.id);
		namestrcpy(objname, NameStr(cagg->data.user_view_name));
	}
	else
	{
		ereport(ERROR,
				(errcode(ERRCODE_INTERNAL_ERROR),
				 errmsg("unexpected hypertable status for %s %d",
						NameStr(ht->fd.table_name),
						status)));
	}
}

static void
compresschunkcxt_init(CompressChunkCxt *cxt, Cache *hcache, Oid hypertable_relid, Oid chunk_relid)
{
	Hypertable *srcht = ts_hypertable_cache_get_entry(hcache, hypertable_relid, CACHE_FLAG_NONE);
	Hypertable *compress_ht;
	Chunk *srcchunk;

	ts_hypertable_permissions_check(srcht->main_table_relid, GetUserId());

	if (!TS_HYPERTABLE_HAS_COMPRESSION_TABLE(srcht))
	{
		NameData cagg_ht_name;
		get_hypertable_or_cagg_name(srcht, &cagg_ht_name);
		ereport(ERROR,
				(errcode(ERRCODE_FEATURE_NOT_SUPPORTED),
				 errmsg("compression not enabled on \"%s\"", NameStr(cagg_ht_name)),
				 errdetail("It is not possible to compress chunks on a hypertable or"
						   " continuous aggregate that does not have compression enabled."),
				 errhint("Enable compression using ALTER TABLE/MATERIALIZED VIEW with"
						 " the timescaledb.compress option.")));
	}
	compress_ht = ts_hypertable_get_by_id(srcht->fd.compressed_hypertable_id);
	if (compress_ht == NULL)
		ereport(ERROR, (errcode(ERRCODE_INTERNAL_ERROR), errmsg("missing compress hypertable")));
	/* user has to be the owner of the compression table too */
	ts_hypertable_permissions_check(compress_ht->main_table_relid, GetUserId());

	if (!srcht->space) /* something is wrong */
		ereport(ERROR,
				(errcode(ERRCODE_INTERNAL_ERROR), errmsg("missing hyperspace for hypertable")));
	/* refetch the srcchunk with all attributes filled in */
	srcchunk = ts_chunk_get_by_relid(chunk_relid, true);
	ts_chunk_validate_chunk_status_for_operation(srcchunk, CHUNK_COMPRESS, true);
	cxt->srcht = srcht;
	cxt->compress_ht = compress_ht;
	cxt->srcht_chunk = srcchunk;
}

static Chunk *
find_chunk_to_merge_into(Hypertable *ht, Chunk *current_chunk)
{
	int64 max_chunk_interval, current_chunk_interval = 0, compressed_chunk_interval = 0;
	Chunk *previous_chunk;
	Point *p;

	const Dimension *time_dim = hyperspace_get_open_dimension(ht->space, 0);

	Assert(time_dim != NULL);

	if (time_dim->fd.compress_interval_length == 0)
		return NULL;

	Assert(current_chunk->cube->num_slices > 0);
	Assert(current_chunk->cube->slices[0]->fd.dimension_id == time_dim->fd.id);

	max_chunk_interval = time_dim->fd.compress_interval_length;

	p = ts_point_create(current_chunk->cube->num_slices);

	/* First coordinate is the time coordinates and we want it to fall into previous chunk
	 * hence we reduce it by 1
	 */
	p->coordinates[p->num_coords++] = current_chunk->cube->slices[0]->fd.range_start - 1;
	current_chunk_interval = current_chunk->cube->slices[0]->fd.range_end -
							 current_chunk->cube->slices[0]->fd.range_start;

	for (int i = p->num_coords; i < current_chunk->cube->num_slices; i++)
	{
		p->coordinates[p->num_coords++] = current_chunk->cube->slices[i]->fd.range_start;
	}

	previous_chunk = ts_hypertable_find_chunk_for_point(ht, p);

	/* If there is no previous adjacent chunk along the time dimension or
	 * if it hasn't been compressed yet, we can't merge.
	 */
	if (!previous_chunk || !OidIsValid(previous_chunk->fd.compressed_chunk_id))
		return NULL;

	Assert(previous_chunk->cube->num_slices > 0);
	Assert(previous_chunk->cube->slices[0]->fd.dimension_id == time_dim->fd.id);

	compressed_chunk_interval = previous_chunk->cube->slices[0]->fd.range_end -
								previous_chunk->cube->slices[0]->fd.range_start;

	/* If the slices do not match (except on time dimension), we cannot merge the chunks. */
	if (previous_chunk->cube->num_slices != current_chunk->cube->num_slices)
		return NULL;

	for (int i = 1; i < previous_chunk->cube->num_slices; i++)
	{
		if (previous_chunk->cube->slices[i]->fd.id != current_chunk->cube->slices[i]->fd.id)
		{
			return NULL;
		}
	}

	/* If the compressed chunk is full, we can't merge any more. */
	if (compressed_chunk_interval == 0 ||
		compressed_chunk_interval + current_chunk_interval > max_chunk_interval)
		return NULL;

	return previous_chunk;
}

/* Check if compression order is violated by merging in a new chunk
 * Because data merged in uses higher sequence numbers than any data already in the chunk,
 * the only way the order is guaranteed can be if we know the data we are merging in would come
 * after the existing data according to the compression order. This is true if the data being merged
 * in has timestamps greater than the existing data and the first column in the order by is time
 * ASC.
 */
static bool
check_is_chunk_order_violated_by_merge(
	const Dimension *time_dim, Chunk *mergable_chunk, Chunk *compressed_chunk,
	const FormData_hypertable_compression **column_compression_info, int num_compression_infos)
{
	const DimensionSlice *mergable_slice =
		ts_hypercube_get_slice_by_dimension_id(mergable_chunk->cube, time_dim->fd.id);
	if (!mergable_slice)
		elog(ERROR, "mergable chunk has no time dimension slice");
	const DimensionSlice *compressed_slice =
		ts_hypercube_get_slice_by_dimension_id(compressed_chunk->cube, time_dim->fd.id);
	if (!compressed_slice)
		elog(ERROR, "compressed chunk has no time dimension slice");

	if (mergable_slice->fd.range_start > compressed_slice->fd.range_start &&
		mergable_slice->fd.range_end > compressed_slice->fd.range_start)
	{
		return true;
	}

	for (int i = 0; i < num_compression_infos; i++)
	{
		if (column_compression_info[i]->orderby_column_index == 1)
		{
			if (!column_compression_info[i]->orderby_asc)
			{
				return true;
			}
			if (get_attnum(time_dim->main_table_relid,
						   NameStr(column_compression_info[i]->attname)) != time_dim->column_attno)
			{
				return true;
			}
		}
	}

	return false;
}

static Oid
compress_chunk_impl(Oid hypertable_relid, Oid chunk_relid)
{
	Oid result_chunk_id = chunk_relid;
	CompressChunkCxt cxt;
	Chunk *compress_ht_chunk, *mergable_chunk;
	Cache *hcache;
	ListCell *lc;
	List *htcols_list = NIL;
	const ColumnCompressionInfo **colinfo_array;
	int i = 0, htcols_listlen;
	RelationSize before_size, after_size;
	CompressionStats cstat;
	bool new_compressed_chunk = false;

	hcache = ts_hypertable_cache_pin();
	compresschunkcxt_init(&cxt, hcache, hypertable_relid, chunk_relid);

	/* acquire locks on src and compress hypertable and src chunk */
	LockRelationOid(cxt.srcht->main_table_relid, AccessShareLock);
	LockRelationOid(cxt.compress_ht->main_table_relid, AccessShareLock);
	LockRelationOid(cxt.srcht_chunk->table_id, ExclusiveLock);

	/* acquire locks on catalog tables to keep till end of txn */
	LockRelationOid(catalog_get_table_id(ts_catalog_get(), HYPERTABLE_COMPRESSION),
					AccessShareLock);
	LockRelationOid(catalog_get_table_id(ts_catalog_get(), CHUNK), RowExclusiveLock);

	DEBUG_WAITPOINT("compress_chunk_impl_start");

	/*
	 * Re-read the state of the chunk after all locks have been acquired and ensure
	 * it is still uncompressed. Another process running in parallel might have
	 * already performed the compression while we were waiting for the locks to be
	 * acquired.
	 */
	Chunk *chunk_state_after_lock = ts_chunk_get_by_relid(chunk_relid, true);

	/* Throw error if chunk has invalid status for operation */
	ts_chunk_validate_chunk_status_for_operation(chunk_state_after_lock, CHUNK_COMPRESS, true);

	/* get compression properties for hypertable */
	htcols_list = ts_hypertable_compression_get(cxt.srcht->fd.id);
	htcols_listlen = list_length(htcols_list);
	mergable_chunk = find_chunk_to_merge_into(cxt.srcht, cxt.srcht_chunk);
	if (!mergable_chunk)
	{
		/* create compressed chunk and a new table */
		compress_ht_chunk = create_compress_chunk(cxt.compress_ht, cxt.srcht_chunk, InvalidOid);
		new_compressed_chunk = true;
	}
	else
	{
		/* use an existing compressed chunk to compress into */
		compress_ht_chunk = ts_chunk_get_by_id(mergable_chunk->fd.compressed_chunk_id, true);
		result_chunk_id = mergable_chunk->table_id;
	}
	/* convert list to array of pointers for compress_chunk */
	colinfo_array = palloc(sizeof(ColumnCompressionInfo *) * htcols_listlen);
	foreach (lc, htcols_list)
	{
		FormData_hypertable_compression *fd = (FormData_hypertable_compression *) lfirst(lc);
		colinfo_array[i++] = fd;
	}
	before_size = ts_relation_size_impl(cxt.srcht_chunk->table_id);
	cstat = compress_chunk(cxt.srcht_chunk->table_id,
						   compress_ht_chunk->table_id,
						   colinfo_array,
						   htcols_listlen);

	/* Drop all FK constraints on the uncompressed chunk. This is needed to allow
	 * cascading deleted data in FK-referenced tables, while blocking deleting data
	 * directly on the hypertable or chunks.
	 */
	ts_chunk_drop_fks(cxt.srcht_chunk);
	after_size = ts_relation_size_impl(compress_ht_chunk->table_id);

	if (new_compressed_chunk)
	{
		compression_chunk_size_catalog_insert(cxt.srcht_chunk->fd.id,
											  &before_size,
											  compress_ht_chunk->fd.id,
											  &after_size,
											  cstat.rowcnt_pre_compression,
											  cstat.rowcnt_post_compression);

		/* Copy chunk constraints (including fkey) to compressed chunk.
		 * Do this after compressing the chunk to avoid holding strong, unnecessary locks on the
		 * referenced table during compression.
		 */
		ts_chunk_constraints_create(cxt.compress_ht, compress_ht_chunk);
		ts_trigger_create_all_on_chunk(compress_ht_chunk);
		ts_chunk_set_compressed_chunk(cxt.srcht_chunk, compress_ht_chunk->fd.id);
	}
	else
	{
		compression_chunk_size_catalog_update_merged(mergable_chunk->fd.id,
													 &before_size,
													 compress_ht_chunk->fd.id,
													 &after_size,
													 cstat.rowcnt_pre_compression,
													 cstat.rowcnt_post_compression);

		const Dimension *time_dim = hyperspace_get_open_dimension(cxt.srcht->space, 0);
		Assert(time_dim != NULL);

		bool chunk_unordered = check_is_chunk_order_violated_by_merge(time_dim,
																	  mergable_chunk,
																	  cxt.srcht_chunk,
																	  colinfo_array,
																	  htcols_listlen);

		ts_chunk_merge_on_dimension(cxt.srcht, mergable_chunk, cxt.srcht_chunk, time_dim->fd.id);

		if (chunk_unordered)
		{
			ts_chunk_set_unordered(mergable_chunk);
			tsl_recompress_chunk_wrapper(mergable_chunk);
		}
	}

	ts_cache_release(hcache);
	return result_chunk_id;
}

static bool
decompress_chunk_impl(Oid uncompressed_hypertable_relid, Oid uncompressed_chunk_relid,
					  bool if_compressed)
{
	Cache *hcache;
	Hypertable *uncompressed_hypertable =
		ts_hypertable_cache_get_cache_and_entry(uncompressed_hypertable_relid,
												CACHE_FLAG_NONE,
												&hcache);
	Hypertable *compressed_hypertable;
	Chunk *uncompressed_chunk;
	Chunk *compressed_chunk;

	ts_hypertable_permissions_check(uncompressed_hypertable->main_table_relid, GetUserId());

	compressed_hypertable =
		ts_hypertable_get_by_id(uncompressed_hypertable->fd.compressed_hypertable_id);
	if (compressed_hypertable == NULL)
		ereport(ERROR, (errcode(ERRCODE_INTERNAL_ERROR), errmsg("missing compressed hypertable")));

	uncompressed_chunk = ts_chunk_get_by_relid(uncompressed_chunk_relid, true);
	if (uncompressed_chunk == NULL)
		ereport(ERROR,
				(errcode(ERRCODE_OBJECT_NOT_IN_PREREQUISITE_STATE),
				 errmsg("table \"%s\" is not a chunk", get_rel_name(uncompressed_chunk_relid))));

	if (uncompressed_chunk->fd.hypertable_id != uncompressed_hypertable->fd.id)
		elog(ERROR, "hypertable and chunk do not match");

	if (uncompressed_chunk->fd.compressed_chunk_id == INVALID_CHUNK_ID)
	{
		ts_cache_release(hcache);
		ereport((if_compressed ? NOTICE : ERROR),
				(errcode(ERRCODE_OBJECT_NOT_IN_PREREQUISITE_STATE),
				 errmsg("chunk \"%s\" is not compressed", get_rel_name(uncompressed_chunk_relid))));
		return false;
	}

	ts_chunk_validate_chunk_status_for_operation(uncompressed_chunk, CHUNK_DECOMPRESS, true);
	compressed_chunk = ts_chunk_get_by_id(uncompressed_chunk->fd.compressed_chunk_id, true);

	/* acquire locks on src and compress hypertable and src chunk */
	LockRelationOid(uncompressed_hypertable->main_table_relid, AccessShareLock);
	LockRelationOid(compressed_hypertable->main_table_relid, AccessShareLock);

	/*
	 * Acquire an ExclusiveLock on the uncompressed and the compressed
	 * chunk (the chunks can still be accessed by reads).
	 *
	 * The lock on the compressed chunk is needed because it gets deleted
	 * after decompression. The lock on the uncompressed chunk is needed
	 * to avoid deadlocks (e.g., caused by later lock upgrades or parallel
	 * started chunk compressions).
	 *
	 * Note: Also the function decompress_chunk() will request an
	 *       ExclusiveLock on the compressed and on the uncompressed
	 *       chunk. See the comments in function about the concurrency of
	 *       operations.
	 */
	LockRelationOid(uncompressed_chunk->table_id, ExclusiveLock);
	LockRelationOid(compressed_chunk->table_id, ExclusiveLock);

	/* acquire locks on catalog tables to keep till end of txn */
	LockRelationOid(catalog_get_table_id(ts_catalog_get(), HYPERTABLE_COMPRESSION),
					AccessShareLock);

	LockRelationOid(catalog_get_table_id(ts_catalog_get(), CHUNK), RowExclusiveLock);

	DEBUG_WAITPOINT("decompress_chunk_impl_start");

	/*
	 * Re-read the state of the chunk after all locks have been acquired and ensure
	 * it is still compressed. Another process running in parallel might have
	 * already performed the decompression while we were waiting for the locks to be
	 * acquired.
	 */
	Chunk *chunk_state_after_lock = ts_chunk_get_by_relid(uncompressed_chunk_relid, true);

	/* Throw error if chunk has invalid status for operation */
	ts_chunk_validate_chunk_status_for_operation(chunk_state_after_lock, CHUNK_DECOMPRESS, true);

	decompress_chunk(compressed_chunk->table_id, uncompressed_chunk->table_id);

	/* Recreate FK constraints, since they were dropped during compression. */
	ts_chunk_create_fks(uncompressed_hypertable, uncompressed_chunk);

	/* Delete the compressed chunk */
	ts_compression_chunk_size_delete(uncompressed_chunk->fd.id);
	ts_chunk_clear_compressed_chunk(uncompressed_chunk);

	/*
	 * Lock the compressed chunk that is going to be deleted. At this point,
	 * the reference to the compressed chunk is already removed from the
	 * catalog. So, new readers do not include it in their operations.
	 *
	 * Note: Calling performMultipleDeletions in chunk_index_tuple_delete
	 * also requests an AccessExclusiveLock on the compressed_chunk. However,
	 * this call makes the lock on the chunk explicit.
	 */
	LockRelationOid(compressed_chunk->table_id, AccessExclusiveLock);
	ts_chunk_drop(compressed_chunk, DROP_RESTRICT, -1);
	ts_cache_release(hcache);
	return true;
}

/*
 * Set if_not_compressed to true for idempotent operation. Aborts transaction if the chunk is
 * already compressed, unless it is running in idempotent mode.
 */

Oid
tsl_compress_chunk_wrapper(Chunk *chunk, bool if_not_compressed)
{
	if (chunk->fd.compressed_chunk_id != INVALID_CHUNK_ID)
	{
		ereport((if_not_compressed ? NOTICE : ERROR),
				(errcode(ERRCODE_DUPLICATE_OBJECT),
				 errmsg("chunk \"%s\" is already compressed", get_rel_name(chunk->table_id))));
		return chunk->table_id;
	}

	return compress_chunk_impl(chunk->hypertable_relid, chunk->table_id);
}

/*
 * Helper for remote invocation of chunk compression and decompression.
 */
static bool
invoke_compression_func_remotely(FunctionCallInfo fcinfo, const Chunk *chunk)
{
	List *datanodes;
	DistCmdResult *distres;
	bool isnull_result = true;
	Size i;

	Assert(chunk->relkind == RELKIND_FOREIGN_TABLE);
	Assert(chunk->data_nodes != NIL);
	datanodes = ts_chunk_get_data_node_name_list(chunk);
	distres = ts_dist_cmd_invoke_func_call_on_data_nodes(fcinfo, datanodes);

	for (i = 0; i < ts_dist_cmd_response_count(distres); i++)
	{
		const char *node_name;
		bool isnull;
		Datum PG_USED_FOR_ASSERTS_ONLY d;

		d = ts_dist_cmd_get_single_scalar_result_by_index(distres, i, &isnull, &node_name);

		/* Make sure data nodes either (1) all return NULL, or (2) all return
		 * a non-null result. */
		if (i > 0 && isnull_result != isnull)
			elog(ERROR, "inconsistent result from data node \"%s\"", node_name);

		isnull_result = isnull;

		if (!isnull)
		{
			Assert(OidIsValid(DatumGetObjectId(d)));
		}
	}

	ts_dist_cmd_close_response(distres);

	return !isnull_result;
}

static bool
compress_remote_chunk(FunctionCallInfo fcinfo, const Chunk *chunk, bool if_not_compressed)
{
	bool success = invoke_compression_func_remotely(fcinfo, chunk);

	if (!success)
		ereport((if_not_compressed ? NOTICE : ERROR),
				(errcode(ERRCODE_DUPLICATE_OBJECT),
				 errmsg("chunk \"%s\" is already compressed", get_rel_name(chunk->table_id))));

	return success;
}

static bool
decompress_remote_chunk(FunctionCallInfo fcinfo, const Chunk *chunk, bool if_compressed)
{
	bool success = invoke_compression_func_remotely(fcinfo, chunk);

	if (!success)
		ereport((if_compressed ? NOTICE : ERROR),
				(errcode(ERRCODE_DUPLICATE_OBJECT),
				 errmsg("chunk \"%s\" is not compressed", get_rel_name(chunk->table_id))));

	return success;
}

/*
 * Create a new compressed chunk using existing table with compressed data.
 *
 * chunk_relid - non-compressed chunk relid
 * chunk_table - table containing compressed data
 */
Datum
tsl_create_compressed_chunk(PG_FUNCTION_ARGS)
{
	Oid chunk_relid = PG_GETARG_OID(0);
	Oid chunk_table = PG_GETARG_OID(1);
	RelationSize uncompressed_size = { .heap_size = PG_GETARG_INT64(2),
									   .toast_size = PG_GETARG_INT64(3),
									   .index_size = PG_GETARG_INT64(4) };
	RelationSize compressed_size = { .heap_size = PG_GETARG_INT64(5),
									 .toast_size = PG_GETARG_INT64(6),
									 .index_size = PG_GETARG_INT64(7) };
	int64 numrows_pre_compression = PG_GETARG_INT64(8);
	int64 numrows_post_compression = PG_GETARG_INT64(9);
	Chunk *chunk;
	Chunk *compress_ht_chunk;
	Cache *hcache;
	CompressChunkCxt cxt;
	bool chunk_was_compressed;

	Assert(!PG_ARGISNULL(0));
	Assert(!PG_ARGISNULL(1));

	ts_feature_flag_check(FEATURE_HYPERTABLE_COMPRESSION);
	TS_PREVENT_FUNC_IF_READ_ONLY();

	chunk = ts_chunk_get_by_relid(chunk_relid, true);
	hcache = ts_hypertable_cache_pin();
	compresschunkcxt_init(&cxt, hcache, chunk->hypertable_relid, chunk_relid);

	/* Acquire locks on src and compress hypertable and src chunk */
	LockRelationOid(cxt.srcht->main_table_relid, AccessShareLock);
	LockRelationOid(cxt.compress_ht->main_table_relid, AccessShareLock);
	LockRelationOid(cxt.srcht_chunk->table_id, ShareLock);

	/* Aquire locks on catalog tables to keep till end of txn */
	LockRelationOid(catalog_get_table_id(ts_catalog_get(), HYPERTABLE_COMPRESSION),
					AccessShareLock);
	LockRelationOid(catalog_get_table_id(ts_catalog_get(), CHUNK), RowExclusiveLock);

	/* Create compressed chunk using existing table */
	compress_ht_chunk = create_compress_chunk(cxt.compress_ht, cxt.srcht_chunk, chunk_table);

	/* Copy chunk constraints (including fkey) to compressed chunk */
	ts_chunk_constraints_create(cxt.compress_ht, compress_ht_chunk);
	ts_trigger_create_all_on_chunk(compress_ht_chunk);

	/* Drop all FK constraints on the uncompressed chunk. This is needed to allow
	 * cascading deleted data in FK-referenced tables, while blocking deleting data
	 * directly on the hypertable or chunks.
	 */
	ts_chunk_drop_fks(cxt.srcht_chunk);

	/* Insert empty stats to compression_chunk_size */
	compression_chunk_size_catalog_insert(cxt.srcht_chunk->fd.id,
										  &uncompressed_size,
										  compress_ht_chunk->fd.id,
										  &compressed_size,
										  numrows_pre_compression,
										  numrows_post_compression);

	chunk_was_compressed = ts_chunk_is_compressed(cxt.srcht_chunk);
	ts_chunk_set_compressed_chunk(cxt.srcht_chunk, compress_ht_chunk->fd.id);
	if (!chunk_was_compressed && ts_table_has_tuples(cxt.srcht_chunk->table_id, AccessShareLock))
	{
		/* The chunk was not compressed before it had the compressed chunk
		 * attached to it, and it contains rows, so we set it to be partial.
		 */
		ts_chunk_set_partial(cxt.srcht_chunk);
	}
	ts_cache_release(hcache);

	PG_RETURN_OID(chunk_relid);
}

Datum
tsl_compress_chunk(PG_FUNCTION_ARGS)
{
	Oid uncompressed_chunk_id = PG_ARGISNULL(0) ? InvalidOid : PG_GETARG_OID(0);
	bool if_not_compressed = PG_ARGISNULL(1) ? false : PG_GETARG_BOOL(1);

	ts_feature_flag_check(FEATURE_HYPERTABLE_COMPRESSION);

	TS_PREVENT_FUNC_IF_READ_ONLY();
	Chunk *chunk = ts_chunk_get_by_relid(uncompressed_chunk_id, true);

	if (chunk->relkind == RELKIND_FOREIGN_TABLE)
	{
		/* chunks of distributed hypertables are foreign tables */
		if (!compress_remote_chunk(fcinfo, chunk, if_not_compressed))
			PG_RETURN_NULL();

		/*
		 * Updating the chunk compression status of the Access Node AFTER executing remote
		 * compression. In the event of failure, the compressed status will NOT be set. The
		 * distributed compression policy will attempt to compress again, which is idempotent, thus
		 * the metadata are eventually consistent.
		 */
		ts_chunk_set_compressed_chunk(chunk, INVALID_CHUNK_ID);
	}
	else
	{
		uncompressed_chunk_id = tsl_compress_chunk_wrapper(chunk, if_not_compressed);
	}

	PG_RETURN_OID(uncompressed_chunk_id);
}

Datum
tsl_decompress_chunk(PG_FUNCTION_ARGS)
{
	Oid uncompressed_chunk_id = PG_ARGISNULL(0) ? InvalidOid : PG_GETARG_OID(0);
	bool if_compressed = PG_ARGISNULL(1) ? false : PG_GETARG_BOOL(1);

	ts_feature_flag_check(FEATURE_HYPERTABLE_COMPRESSION);

	TS_PREVENT_FUNC_IF_READ_ONLY();
	Chunk *uncompressed_chunk = ts_chunk_get_by_relid(uncompressed_chunk_id, true);

	if (NULL == uncompressed_chunk)
		elog(ERROR, "unknown chunk id %d", uncompressed_chunk_id);

	if (uncompressed_chunk->relkind == RELKIND_FOREIGN_TABLE)
	{
		/*
		 * Updating the chunk compression status of the Access Node BEFORE executing remote
		 * decompression. In the event of failure, the compressed status will be cleared. The
		 * distributed compression policy will attempt to compress again, which is idempotent, thus
		 * the metadata are eventually consistent.
		 * If CHUNK_STATUS_COMPRESSED is cleared, then it is probable that a remote compress_chunk()
		 * has not taken place, but not certain. For this above reason, this flag should not be
		 * assumed to be consistent (when it is cleared) for Access-Nodes. When used in distributed
		 * hypertables one should take advantage of the idempotent properties of remote
		 * compress_chunk() and distributed compression policy to make progress.
		 */
		ts_chunk_clear_compressed_chunk(uncompressed_chunk);

		if (!decompress_remote_chunk(fcinfo, uncompressed_chunk, if_compressed))
			PG_RETURN_NULL();

		PG_RETURN_OID(uncompressed_chunk_id);
	}

	if (!decompress_chunk_impl(uncompressed_chunk->hypertable_relid,
							   uncompressed_chunk_id,
							   if_compressed))
		PG_RETURN_NULL();

	PG_RETURN_OID(uncompressed_chunk_id);
}

bool
tsl_recompress_chunk_wrapper(Chunk *uncompressed_chunk)
{
	Oid uncompressed_chunk_relid = uncompressed_chunk->table_id;
	if (ts_chunk_is_unordered(uncompressed_chunk))
	{
		if (!decompress_chunk_impl(uncompressed_chunk->hypertable_relid,
								   uncompressed_chunk_relid,
								   false))
			return false;
	}
	Chunk *chunk = ts_chunk_get_by_relid(uncompressed_chunk_relid, true);
	Assert(!ts_chunk_is_compressed(chunk));
	tsl_compress_chunk_wrapper(chunk, false);
	return true;
}

static bool
recompress_remote_chunk(FunctionCallInfo fcinfo, Chunk *chunk)
{
	return invoke_compression_func_remotely(fcinfo, chunk);
}

/*
 * This is hacky but it doesn't matter. We just want to check for the existence of such an index
 * on the compressed chunk. For distributed hypertables, returning the index oid would raise issues,
 * because the Access Node does not see that oid. So we return the oid of the uncompresed chunk
 * instead, when an index is found.
 */
extern Datum
tsl_get_compressed_chunk_index_for_recompression(PG_FUNCTION_ARGS)
{
	Oid uncompressed_chunk_id = PG_ARGISNULL(0) ? InvalidOid : PG_GETARG_OID(0);
	Chunk *uncompressed_chunk = ts_chunk_get_by_relid(uncompressed_chunk_id, true);

	ts_feature_flag_check(FEATURE_HYPERTABLE_COMPRESSION);
	if (NULL == uncompressed_chunk)
		elog(ERROR, "unknown chunk id %d", uncompressed_chunk_id);

	/* push down to data nodes for distributed case */
	if (uncompressed_chunk->relkind == RELKIND_FOREIGN_TABLE)
	{
		/* look for index on data nodes */
		if (!(invoke_compression_func_remotely(fcinfo, uncompressed_chunk)))
			PG_RETURN_NULL();
		else // don't care what the idx oid is, data node will find it and open it
			PG_RETURN_OID(uncompressed_chunk_id);
	}
	int32 srcht_id = uncompressed_chunk->fd.hypertable_id;
	Chunk *compressed_chunk = ts_chunk_get_by_id(uncompressed_chunk->fd.compressed_chunk_id, true);

	List *htcols_list = ts_hypertable_compression_get(srcht_id);
	ListCell *lc;
	int htcols_listlen = list_length(htcols_list);

	const ColumnCompressionInfo **colinfo_array;
	colinfo_array = palloc(sizeof(ColumnCompressionInfo *) * htcols_listlen);

	int i = 0;

	foreach (lc, htcols_list)
	{
		FormData_hypertable_compression *fd = (FormData_hypertable_compression *) lfirst(lc);
		colinfo_array[i++] = fd;
	}

	const ColumnCompressionInfo **keys;
	int n_keys;
	int16 *in_column_offsets = compress_chunk_populate_keys(uncompressed_chunk->table_id,
															colinfo_array,
															htcols_listlen,
															&n_keys,
															&keys);

	Relation uncompressed_chunk_rel = table_open(uncompressed_chunk->table_id, ExclusiveLock);
	Relation compressed_chunk_rel = table_open(compressed_chunk->table_id, ExclusiveLock);
	TupleDesc compressed_rel_tupdesc = RelationGetDescr(compressed_chunk_rel);
	TupleDesc uncompressed_rel_tupdesc = RelationGetDescr(uncompressed_chunk_rel);

	RowCompressor row_compressor;
	row_compressor_init(&row_compressor,
						uncompressed_rel_tupdesc,
						compressed_chunk_rel,
						htcols_listlen,
						colinfo_array,
						in_column_offsets,
						compressed_rel_tupdesc->natts,
						true /*need_bistate*/,
						true /*reset_sequence*/,
						RECOMPRESS);

	/*
	 * Keep the ExclusiveLock on the compressed chunk. This lock will be requested
	 * by recompression later on, both in the case of segmentwise recompression, and
	 * in the case of decompress-compress. This implicitly locks the index too, so
	 * it cannot be dropped in another session, which is what we want to prevent by
	 * locking the compressed chunk here
	 */
	table_close(compressed_chunk_rel, NoLock);
	table_close(uncompressed_chunk_rel, NoLock);

	row_compressor_finish(&row_compressor);

	if (OidIsValid(row_compressor.index_oid))
	{
		PG_RETURN_OID(uncompressed_chunk_id);
	}
	else
		PG_RETURN_NULL();
}

/*
 * Recompress an existing chunk by decompressing the batches
 * that are affected by the addition of newer data. The existing
 * compressed chunk will not be recreated but modified in place.
 *
 * 0 uncompressed_chunk_id REGCLASS
 * 1 if_not_compressed BOOL = false
 */
Datum
tsl_recompress_chunk_segmentwise(PG_FUNCTION_ARGS)
{
	Oid uncompressed_chunk_id = PG_ARGISNULL(0) ? InvalidOid : PG_GETARG_OID(0);
	bool if_not_compressed = PG_ARGISNULL(1) ? false : PG_GETARG_BOOL(1);

	ts_feature_flag_check(FEATURE_HYPERTABLE_COMPRESSION);
	TS_PREVENT_FUNC_IF_READ_ONLY();
	Chunk *uncompressed_chunk = ts_chunk_get_by_relid(uncompressed_chunk_id, true);

	/* need to push down to data nodes if this is an access node */
	if (uncompressed_chunk->relkind == RELKIND_FOREIGN_TABLE)
	{
		if (!recompress_remote_chunk(fcinfo, uncompressed_chunk))
			PG_RETURN_NULL();

		PG_RETURN_OID(uncompressed_chunk_id);
	}

	int32 status = uncompressed_chunk->fd.status;

	if (status == CHUNK_STATUS_DEFAULT)
		elog(ERROR, "call compress_chunk instead of recompress_chunk");
	if (status == CHUNK_STATUS_COMPRESSED)
	{
		int elevel = if_not_compressed ? NOTICE : ERROR;
		elog(elevel,
			 "nothing to recompress in chunk %s.%s",
			 uncompressed_chunk->fd.schema_name.data,
			 uncompressed_chunk->fd.table_name.data);
	}
	/*
	 * only proceed if status in (3, 9, 11)
	 * 1: compressed
	 * 2: compressed_unordered
	 * 4: frozen
	 * 8: compressed_partial
	 */
	if (!(ts_chunk_is_compressed(uncompressed_chunk) &&
		  (ts_chunk_is_unordered(uncompressed_chunk) || ts_chunk_is_partial(uncompressed_chunk))))
		elog(ERROR,
			 "unexpected chunk status %d in chunk %s.%s",
			 status,
			 uncompressed_chunk->fd.schema_name.data,
			 uncompressed_chunk->fd.table_name.data);

	Chunk *compressed_chunk = ts_chunk_get_by_id(uncompressed_chunk->fd.compressed_chunk_id, true);

	/* new status after recompress should simply be compressed (1)
	 * It is ok to update this early on in the transaction as it keeps a lock
	 * on the updated tuple in the CHUNK table potentially preventing other transaction
	 * from updating it
	 */
	ts_chunk_clear_status(uncompressed_chunk,
						  CHUNK_STATUS_COMPRESSED_UNORDERED | CHUNK_STATUS_COMPRESSED_PARTIAL);

	/* lock both chunks, compresssed and uncompressed */
	/* TODO: Take RowExclusive locks instead of AccessExclusive */
	LockRelationOid(uncompressed_chunk->table_id, ExclusiveLock);
	LockRelationOid(compressed_chunk->table_id, ExclusiveLock);
	Relation compressed_chunk_rel = table_open(compressed_chunk->table_id, ExclusiveLock);
	Relation uncompressed_chunk_rel = table_open(uncompressed_chunk->table_id, ExclusiveLock);

	recompress_partial_chunks(uncompressed_chunk, compressed_chunk);

#if PG14_LT
	int options = 0;
#else
	ReindexParams params = { 0 };
	ReindexParams *options = &params;
#endif
	reindex_relation(compressed_chunk->table_id, 0, options);

	/* changed chunk status, so invalidate any plans involving this chunk */
	CacheInvalidateRelcacheByRelid(uncompressed_chunk_id);
	table_close(uncompressed_chunk_rel, ExclusiveLock);
	table_close(compressed_chunk_rel, ExclusiveLock);

	PG_RETURN_OID(uncompressed_chunk_id);
}


/* Helper method to update current segment which is being recompressed */
static void
update_current_segment(CompressedSegmentInfo **current_segment, TupleTableSlot *slot,
					   int nsegmentby_cols)
{
	Datum val;
	bool is_null;
	int seg_idx = 0;
	for (int i = 0; i < nsegmentby_cols; i++)
	{
		int16 col_offset = current_segment[i]->decompressed_chunk_offset;
		val = slot_getattr(slot, AttrOffsetGetAttrNumber(col_offset), &is_null);
		if (!segment_info_datum_is_in_group(current_segment[seg_idx++]->segment_info, val, is_null))
		{
			/* new segment, need to do per-segment processing */
			pfree(current_segment[seg_idx - 1]->segment_info); /* because increased previously */
			SegmentInfo *segment_info =
				segment_info_new(TupleDescAttr(slot->tts_tupleDescriptor, col_offset));
			segment_info_update(segment_info, val, is_null);
			current_segment[seg_idx - 1]->segment_info = segment_info;
			current_segment[seg_idx - 1]->decompressed_chunk_offset = col_offset;
		}
	}
}

/* Helper method to find if segment being recompressed, has encountered a new segment */
static bool
is_segment_changed(CompressedSegmentInfo **current_segment, TupleTableSlot *slot,
				   int nsegmentby_cols)
{
	Datum val;
	bool is_null;
	bool changed_segment = false;
	for (int i = 0; i < nsegmentby_cols; i++)
	{
		int16 col_offset = current_segment[i]->decompressed_chunk_offset;
		val = slot_getattr(slot, AttrOffsetGetAttrNumber(col_offset), &is_null);
		if (!segment_info_datum_is_in_group(current_segment[i++]->segment_info, val, is_null))
		{
			changed_segment = true;
			break;
		}
	}
	return changed_segment;
}

/* This is a wrapper around row_compressor_append_sorted_rows. */
static void
recompress_segment(Tuplesortstate *tuplesortstate, Relation compressed_chunk_rel,
				   RowCompressor *row_compressor)
{
	row_compressor_append_sorted_rows(row_compressor,
									  tuplesortstate,
									  RelationGetDescr(compressed_chunk_rel));
	/* make changes visible */
	CommandCounterIncrement();
}

/*
 * This function updates catalog chunk compression statistics
 * for an existing compressed chunk after it has been recompressed
 * segmentwise in-place (as opposed to creating a new compressed chunk).
 * Note that because of this it is not possible to accurately report
 * the fields
 * uncompressed_chunk_size, uncompressed_index_size, uncompressed_toast_size
 * anymore, so these are not updated.
 */
static int
compression_chunk_size_catalog_update_recompressed(int32 uncompressed_chunk_id,
												   int32 compressed_chunk_id,
												   const RelationSize *recompressed_size,
												   int64 rowcnt_pre_compression,
												   int64 rowcnt_post_compression)
{
	ScanIterator iterator =
		ts_scan_iterator_create(COMPRESSION_CHUNK_SIZE, RowExclusiveLock, CurrentMemoryContext);
	bool updated = false;

	iterator.ctx.index =
		catalog_get_index(ts_catalog_get(), COMPRESSION_CHUNK_SIZE, COMPRESSION_CHUNK_SIZE_PKEY);
	ts_scan_iterator_scan_key_init(&iterator,
								   Anum_compression_chunk_size_pkey_chunk_id,
								   BTEqualStrategyNumber,
								   F_INT4EQ,
								   Int32GetDatum(uncompressed_chunk_id));
	ts_scanner_foreach(&iterator)
	{
		Datum values[Natts_compression_chunk_size];
		bool replIsnull[Natts_compression_chunk_size] = { false };
		bool repl[Natts_compression_chunk_size] = { false };
		bool should_free;
		TupleInfo *ti = ts_scan_iterator_tuple_info(&iterator);
		HeapTuple tuple = ts_scanner_fetch_heap_tuple(ti, false, &should_free);
		HeapTuple new_tuple = NULL;
		heap_deform_tuple(tuple, ts_scanner_get_tupledesc(ti), values, replIsnull);

		/* Only update the information pertaining to the compressed chunk */
		/* these fields are about the compressed chunk so they can be updated */
		values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_compressed_heap_size)] =
			Int64GetDatum(recompressed_size->heap_size);
		repl[AttrNumberGetAttrOffset(Anum_compression_chunk_size_compressed_heap_size)] = true;

		values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_compressed_toast_size)] =
			Int64GetDatum(recompressed_size->toast_size);
		repl[AttrNumberGetAttrOffset(Anum_compression_chunk_size_compressed_toast_size)] = true;

		values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_compressed_index_size)] =
			Int64GetDatum(recompressed_size->index_size);
		repl[AttrNumberGetAttrOffset(Anum_compression_chunk_size_compressed_index_size)] = true;

		values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_numrows_pre_compression)] =
			Int64GetDatum(rowcnt_pre_compression);
		repl[AttrNumberGetAttrOffset(Anum_compression_chunk_size_numrows_pre_compression)] = true;

		values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_numrows_post_compression)] =
			Int64GetDatum(rowcnt_post_compression);
		repl[AttrNumberGetAttrOffset(Anum_compression_chunk_size_numrows_post_compression)] = true;

		new_tuple =
			heap_modify_tuple(tuple, ts_scanner_get_tupledesc(ti), values, replIsnull, repl);
		ts_catalog_update(ti->scanrel, new_tuple);
		heap_freetuple(new_tuple);

		if (should_free)
			heap_freetuple(tuple);

		updated = true;
		break;
	}

	ts_scan_iterator_end(&iterator);
	ts_scan_iterator_close(&iterator);
	return updated;
}

/*
 * Helper method to get statistics of compressed chunk, before recompression.
 */
static void
get_compressed_chunk_statistics(Relation in_rel, Oid index_oid, CompressedChunkStats *stats)
{
	Relation idxrel;
	IndexScanDesc index_scan;
	TupleTableSlot *heap_tuple_slot = NULL;
	AttrNumber metacount_attnum;
	AttrNumber seq_attnum;
	Datum curr_seqnum_val = SEQUENCE_NUM_GAP;
	Datum prev_seqnum_val = SEQUENCE_NUM_GAP;
	unsigned long total_decompressed_rows = 0;
	unsigned long total_compressed_rows = 0;
	unsigned long total_expected_gaps = 0;
	unsigned long total_gaps_in_sequences = 0;
	unsigned long total_batches_per_segment = 0;
	unsigned long total_segments = 0;

	idxrel = index_open(index_oid, AccessShareLock);
	index_scan = index_beginscan(in_rel, idxrel, GetTransactionSnapshot(), 0, 0);
	index_rescan(index_scan, NULL, 0, NULL, 0);
	heap_tuple_slot = table_slot_create(in_rel, NULL);
	metacount_attnum = get_attnum(in_rel->rd_id, COMPRESSION_COLUMN_METADATA_COUNT_NAME);
	seq_attnum = get_attnum(in_rel->rd_id, COMPRESSION_COLUMN_METADATA_SEQUENCE_NUM_NAME);

	while (index_getnext_slot(index_scan, ForwardScanDirection, heap_tuple_slot))
	{
		slot_getallattrs(heap_tuple_slot);
		total_decompressed_rows += heap_tuple_slot->tts_values[metacount_attnum - 1];
		total_compressed_rows++;
		curr_seqnum_val = heap_tuple_slot->tts_values[seq_attnum - 1];
		if (curr_seqnum_val > SEQUENCE_NUM_GAP)
		{
			total_gaps_in_sequences += ((curr_seqnum_val - prev_seqnum_val) - 1);
			total_expected_gaps += 9;
		}

		if (prev_seqnum_val > curr_seqnum_val)
		{
			/* there was change in segments */
			prev_seqnum_val = curr_seqnum_val = SEQUENCE_NUM_GAP;
			stats->num_batches_per_segments[total_segments] = total_batches_per_segment;
			total_segments++;
			total_batches_per_segment = 1;
		}
		else
		{
			total_batches_per_segment++;
			prev_seqnum_val = curr_seqnum_val;
		}
	}
	index_endscan(index_scan);
	index_close(idxrel, AccessShareLock);
	ExecDropSingleTupleTableSlot(heap_tuple_slot);

	stats->num_rows_pre_compression = total_decompressed_rows;
	stats->num_rows_post_compression = total_compressed_rows;
	stats->num_expected_gaps_in_sequences = total_expected_gaps;
	stats->num_gaps_in_sequences = total_gaps_in_sequences;
	stats->num_segments = total_segments + 1;
	stats->num_batches_per_segments[total_segments] = total_batches_per_segment;
}

/* Helper method to initial the first segment by column values in current_segment */
static void
initialize_segment(RowDecompressor *decompressor, CompressedSegmentInfo **current_segment,
				   Chunk *uncompressed_chunk, TupleTableSlot *uncompressed_slot)
{
	int i = 0;
	Datum val;
	bool is_null;
	SegmentInfo *segment_info = NULL;
	/* initialize current segment */
	for (int col = 0; col < uncompressed_slot->tts_tupleDescriptor->natts; col++)
	{
		if (uncompressed_slot->tts_tupleDescriptor->attrs[col].attisdropped)
			continue;
		val = slot_getattr(uncompressed_slot, AttrOffsetGetAttrNumber(col), &is_null);
		Form_pg_attribute decompressed_attr = TupleDescAttr(decompressor->out_desc, col);
		char *col_name = NameStr(decompressed_attr->attname);
		FormData_hypertable_compression *fd =
			ts_hypertable_compression_get_by_pkey(uncompressed_chunk->fd.hypertable_id, col_name);
		if (COMPRESSIONCOL_IS_SEGMENT_BY(fd))
		{
			segment_info =
				segment_info_new(TupleDescAttr(uncompressed_slot->tts_tupleDescriptor, col));
			current_segment[i]->decompressed_chunk_offset = col;
			/* also need to call segment_info_update here to update the val part */
			segment_info_update(segment_info, val, is_null);
			current_segment[i]->segment_info = segment_info;
			i++;
		}
	}
}

/* initialize tuplesort state */
static CompressTuplesortstateCxt *
initialize_tuplestore(Relation uncompressed_chunk_rel, const ColumnCompressionInfo **keys,
					  int n_keys)
{
	CompressTuplesortstateCxt *tuplestorecxt = NULL;
	int tuplesortopts = 0;
#if PG15_GE
	tuplesortopts = TUPLESORT_NONE | TUPLESORT_RANDOMACCESS;
#else
	tuplesortopts = 1;
#endif
	tuplestorecxt = (CompressTuplesortstateCxt *) palloc0(sizeof(CompressTuplesortstateCxt));
	tuplestorecxt->sort_keys = palloc(sizeof(AttrNumber) * n_keys);
	tuplestorecxt->sort_operators = palloc(sizeof(Oid) * n_keys);
	tuplestorecxt->sort_collations = palloc(sizeof(Oid) * n_keys);
	tuplestorecxt->nulls_first = palloc(sizeof(bool) * n_keys);

	for (int n = 0; n < n_keys; n++)
		compress_chunk_populate_sort_info_for_column(RelationGetRelid(uncompressed_chunk_rel),
													 keys[n],
													 &tuplestorecxt->sort_keys[n],
													 &tuplestorecxt->sort_operators[n],
													 &tuplestorecxt->sort_collations[n],
													 &tuplestorecxt->nulls_first[n]);

	tuplestorecxt->tuplestore = tuplesort_begin_heap(RelationGetDescr(uncompressed_chunk_rel),
													 n_keys,
													 tuplestorecxt->sort_keys,
													 tuplestorecxt->sort_operators,
													 tuplestorecxt->sort_collations,
													 tuplestorecxt->nulls_first,
													 maintenance_work_mem,
													 NULL,
													 tuplesortopts);
	return tuplestorecxt;
}

/* cleaup tuplesort state */
static void
cleanup_tuplestorecxt(CompressTuplesortstateCxt *tuplestorecxt)
{
	if (tuplestorecxt)
	{
		tuplesort_end(tuplestorecxt->tuplestore);
		pfree(tuplestorecxt->sort_keys);
		pfree(tuplestorecxt->sort_operators);
		pfree(tuplestorecxt->sort_collations);
		pfree(tuplestorecxt->nulls_first);
		pfree(tuplestorecxt);
		tuplestorecxt = NULL;
	}
}

/*
 * Helper method to resize AffectedSegmentsCxt when total affected
 * compressed tuples exceeds its default value of MAX_ROWS_PER_COMPRESSION
 * This method will double the size of arrays present in AffectedSegmentsCxt
 * and moves contents of existing AffectedSegmentsCxt to new context.
 */
static AffectedSegmentsCxt *
resize_affectedsegmentscxt(AffectedSegmentsCxt *as)
{
	AffectedSegmentsCxt *new_cxt = (AffectedSegmentsCxt *) palloc0(sizeof(AffectedSegmentsCxt));
	unsigned long factor = 0;
	unsigned long new_size = 0;

	if (!as->new_segments_found)
	{
		factor = (as->default_size / MAX_ROWS_PER_COMPRESSION) + 1;
		new_size = factor * MAX_ROWS_PER_COMPRESSION;
		/* copy only affected compressed segments context */
		new_cxt->num_matching_uncompressedrows = palloc0(sizeof(unsigned long) * new_size);
		memmove(new_cxt->num_matching_uncompressedrows,
				as->num_matching_uncompressedrows,
				sizeof(unsigned long) * as->default_size);
		new_cxt->compressed_tuples = palloc0(sizeof(unsigned long) * new_size);
		memmove(new_cxt->compressed_tuples,
				as->compressed_tuples,
				sizeof(unsigned long) * as->default_size);

		new_cxt->num_non_matching_compressedrows = as->num_non_matching_compressedrows;
		new_cxt->default_size = new_size;
		new_cxt->new_segments_default_size = as->new_segments_default_size;
	}
	else
	{
		factor = (as->new_segments_default_size / MAX_ROWS_PER_COMPRESSION) + 1;
		new_size = factor * MAX_ROWS_PER_COMPRESSION;
		/* copy all new compressed segments context */
		new_cxt->num_non_matching_compressedrows = palloc0(sizeof(unsigned long) * new_size);
		memmove(new_cxt->num_non_matching_compressedrows,
				as->num_non_matching_compressedrows,
				sizeof(unsigned long) * as->new_segments_default_size);

		new_cxt->num_matching_uncompressedrows = as->num_matching_uncompressedrows;
		new_cxt->compressed_tuples = as->compressed_tuples;
		new_cxt->new_segments_default_size = new_size;
		new_cxt->default_size = as->default_size;
	}
	/* copy rest of the contents */
	new_cxt->total_affected_compressed_rows = as->total_affected_compressed_rows;
	new_cxt->new_segments_found = as->new_segments_found;
	new_cxt->total_new_compressed_segments = as->total_new_compressed_segments;
	return new_cxt;
}

/*
 * Helper method to fetch all rows from uncompressed chunk,
 * sort these rows and save it in tuplestore
 */
static int
save_uncompressed_rows_in_tuplestore(Chunk *uncompressed_chunk,
									 Tuplesortstate *uncompressed_rows_sortstate)
{
	Relation uncompressed_chunk_rel;
	TableScanDesc heapScan;
	HeapTuple uncompressed_tuple = NULL;
	TupleTableSlot *heap_tuple_slot = NULL;
	int total_uncompressed_rows = 0;

	uncompressed_chunk_rel = RelationIdGetRelation(uncompressed_chunk->table_id);
	heapScan = table_beginscan(uncompressed_chunk_rel, GetLatestSnapshot(), 0, NULL);
	heap_tuple_slot =
		MakeTupleTableSlot(RelationGetDescr(uncompressed_chunk_rel), &TTSOpsHeapTuple);

	while ((uncompressed_tuple = heap_getnext(heapScan, ForwardScanDirection)) != NULL)
	{
		ExecStoreHeapTuple(uncompressed_tuple, heap_tuple_slot, false);
		slot_getallattrs(heap_tuple_slot);
		tuplesort_puttupleslot(uncompressed_rows_sortstate, heap_tuple_slot);
		total_uncompressed_rows++;
	}
	truncate_relation(uncompressed_chunk->table_id);
	ExecDropSingleTupleTableSlot(heap_tuple_slot);
	table_endscan(heapScan);
	RelationClose(uncompressed_chunk_rel);
	/* sort the tuplestore */
	tuplesort_performsort(uncompressed_rows_sortstate);
	return total_uncompressed_rows;
}

/*
 * Helper method used to build scankeys based on segmentby and orderby
 * key columns to do lookup into uncompressed chunks.
 */
static ScanKeyData *
build_segment_order_by_scankeys(RowDecompressor *decompressor, Chunk *uncompressed_chunk,
								TupleTableSlot *slot, Bitmapset **null_columns_count,
								int *num_scankeys, int key_flags)
{
	ScanKeyData *segment_order_by_keys;
	Bitmapset *key_columns = NULL;
	Bitmapset *null_columns = NULL;
	int num_keys = 0;
	for (int i = 0; i < decompressor->out_desc->natts; i++)
	{
		Form_pg_attribute attr = TupleDescAttr(decompressor->out_desc, i);
		if (attr->attisdropped)
			continue;
		FormData_hypertable_compression *fd =
			ts_hypertable_compression_get_by_pkey(uncompressed_chunk->fd.hypertable_id,
												  attr->attname.data);
		if (key_flags & SEGMENTBY_KEYS)
		{
			if (COMPRESSIONCOL_IS_SEGMENT_BY(fd))
				key_columns =
					bms_add_member(key_columns, attr->attnum - FirstLowInvalidHeapAttributeNumber);
		}
		if (key_flags & ORDERBY_KEYS)
		{
			if (COMPRESSIONCOL_IS_ORDER_BY(fd))
				key_columns =
					bms_add_member(key_columns, attr->attnum - FirstLowInvalidHeapAttributeNumber);
		}
	}
	segment_order_by_keys = build_scankeys(uncompressed_chunk->fd.hypertable_id,
										   uncompressed_chunk->table_id,
										   *decompressor,
										   key_columns,
										   &null_columns,
										   slot,
										   &num_keys);
	if (null_columns_count)
		*null_columns_count = null_columns;
	bms_free(key_columns);
	*num_scankeys = num_keys;
	return segment_order_by_keys;
}

/*
 * Helper method to check if segmentby columns in compressed/uncompressed
 * tuples have NULL or not.
 *
 * Return true if all nullable segmentby columns have NULL values, else
 * return false.
 */
static bool
has_all_nulls(RowDecompressor *decompressor, HeapTuple compressed_tuple,
			  HeapTuple uncompressed_tuple, Bitmapset *null_columns)
{
	bool nulls_found = true;
	for (int attno = bms_next_member(null_columns, -1); attno >= 0;
		 attno = bms_next_member(null_columns, attno))
	{
		if (!(heap_attisnull(compressed_tuple, attno, decompressor->in_desc)))
		{
			nulls_found = false;
			break;
		}
		if (!(heap_attisnull(uncompressed_tuple, attno, decompressor->out_desc)))
		{
			nulls_found = false;
			break;
		}
	}
	return nulls_found;
}

/*
 * Helper method to build scankeys to do lookup into compressed chunk index
 */
static ScanKeyData *
build_index_scankeys(Relation in_rel, Relation idxrel, TupleTableSlot *compressedslot,
					 TupleTableSlot *uncompressedslot, int *num_keys, int key_flags)
{
	ScanKeyData *scankey = NULL;
	int indnkeyatts;
	int total_keys = 0;
	int attoff;
	bool isnull;
	Datum indclassDatum;
	oidvector *opclass;
	int2vector *indkey = NULL;

	indkey = &idxrel->rd_index->indkey;
	indclassDatum =
		SysCacheGetAttr(INDEXRELID, idxrel->rd_indextuple, Anum_pg_index_indclass, &isnull);
	Assert(!isnull);
	opclass = (oidvector *) DatumGetPointer(indclassDatum);

	indnkeyatts = IndexRelationGetNumberOfKeyAttributes(idxrel);
	scankey = palloc0(sizeof(ScanKeyData) * indnkeyatts);

	/* Build scankey for every attribute in the index. */
	for (attoff = 0; attoff < indnkeyatts; attoff++)
	{
		Oid operator;
		Oid opfamily;
		RegProcedure regop;
		int pkattno = attoff + 1;
		int mainattno = indkey->values[attoff];
		Oid optype = get_opclass_input_type(opclass->values[attoff]);
		/*
		 * Load the operator info.  We need this to get the equality operator
		 * function for the scan key.
		 */
		opfamily = get_opclass_family(opclass->values[attoff]);
		operator= get_opfamily_member(opfamily, optype, optype, BTEqualStrategyNumber);
		regop = get_opcode(operator);

		if (attoff < indnkeyatts - 1)
		{
			/* Initialize segmentby scankeys. */
			if (key_flags & SEGMENTBY_KEYS)
			{
				if (compressedslot)
				{
					ScanKeyInit(&scankey[attoff],
								pkattno,
								BTEqualStrategyNumber,
								regop,
								compressedslot->tts_values[mainattno - 1]);
					if (compressedslot->tts_isnull[mainattno - 1])
						scankey[attoff].sk_flags |= (SK_ISNULL | SK_SEARCHNULL);
					scankey[attoff].sk_collation = idxrel->rd_indcollation[attoff];
					total_keys++;
				}
				else if (uncompressedslot)
				{
					/* get index attribute name */
					Form_pg_attribute attr = TupleDescAttr(idxrel->rd_att, attoff);
					AttrNumber attno = get_attnum(in_rel->rd_id, attr->attname.data);
					ScanKeyInit(&scankey[attoff],
								pkattno,
								BTEqualStrategyNumber,
								regop,
								uncompressedslot->tts_values[attno - 1]);
					if (uncompressedslot->tts_isnull[attno - 1])
						scankey[attoff].sk_flags |= (SK_ISNULL | SK_SEARCHNULL);
					scankey[attoff].sk_collation = idxrel->rd_indcollation[attoff];
					total_keys++;
				}
			}
		}
		else if (attoff == indnkeyatts - 1 && compressedslot)
		{
			/* Initialize _ts_meta_sequence_num scankeys. */
			if (key_flags & NEXT_SEQNUM_KEYS)
			{
				ScanKeyInit(&scankey[attoff],
							pkattno,
							BTEqualStrategyNumber,
							regop,
							compressedslot->tts_values[mainattno - 1] + SEQUENCE_NUM_GAP);
				scankey[attoff].sk_collation = idxrel->rd_indcollation[attoff];
				total_keys++;
			}
			else if (key_flags & SEQNUM_KEYS)
			{
				ScanKeyInit(&scankey[attoff],
							pkattno,
							BTEqualStrategyNumber,
							regop,
							compressedslot->tts_values[mainattno - 1]);
				scankey[attoff].sk_collation = idxrel->rd_indcollation[attoff];
				total_keys++;
			}
		}
	}
	*num_keys = total_keys;
	return scankey;
}

/*
 * Helper method to fetch next compressed tuple based on segmentby and
 * _ts_meta_sequence_num column values from searchslot
 */
static bool
fetch_next_compressed_tuple_using_index(Relation in_rel, Oid index_oid, TupleTableSlot *searchslot,
										HeapTuple *compressed_tuple)
{
	Relation idxrel;
	IndexScanDesc index_scan;
	ScanKeyData *scankey = NULL;
	TupleTableSlot *index_slot = NULL;
	int num_scankeys = 0;
	bool ret = false;

	idxrel = index_open(index_oid, AccessShareLock);
	scankey = build_index_scankeys(in_rel,
								   idxrel,
								   searchslot,
								   NULL,
								   &num_scankeys,
								   SEGMENTBY_KEYS | NEXT_SEQNUM_KEYS);
	index_scan = index_beginscan(in_rel, idxrel, GetTransactionSnapshot(), num_scankeys, 0);
	index_rescan(index_scan, scankey, num_scankeys, NULL, 0);
	index_slot = table_slot_create(in_rel, NULL);
	if (index_getnext_slot(index_scan, ForwardScanDirection, index_slot))
	{
		slot_getallattrs(index_slot);
		/* found valid tuple */
		*compressed_tuple = heap_form_tuple(index_slot->tts_tupleDescriptor,
											index_slot->tts_values,
											index_slot->tts_isnull);
		(*compressed_tuple)->t_self = index_slot->tts_tid;
		ret = true;
	}
	else
	{
		compressed_tuple = NULL;
	}
	ExecDropSingleTupleTableSlot(index_slot);
	index_endscan(index_scan);
	index_close(idxrel, AccessShareLock);
	return ret;
}

static bool
fetch_next_compressed_tuple(RowDecompressor *decompressor, RowCompressor *row_compressor,
							Chunk *uncompressed_chunk, int nsegmentby_cols,
							TupleTableSlot *compressed_slot, HeapTuple *next_tuple)
{
	bool tuple_found = false;
	HeapTuple next_compressed_tuple = NULL;
	if (OidIsValid(row_compressor->index_oid))
		tuple_found = fetch_next_compressed_tuple_using_index(decompressor->in_rel,
															  row_compressor->index_oid,
															  compressed_slot,
															  &next_compressed_tuple);
	*next_tuple = next_compressed_tuple;
	return tuple_found;
}

/*
 * In a compressed chunk there can be multiple compressed rows with same segment
 * by values and incrementing sequence numbers. This method will tell if the
 * given tuple (searchslot) is the last tuple with highest sequence number.
 *
 * Return true if searchslot is the last tuple else return false.
 */
static bool
is_compressed_tuple_in_last_batch(Relation in_rel, Oid index_oid, TupleTableSlot *searchslot)
{
	Relation idxrel;
	IndexScanDesc index_scan;
	ScanKeyData *scankey = NULL;
	TupleTableSlot *index_slot;
	int num_scankeys = 0;
	bool ret = false;

	idxrel = index_open(index_oid, AccessShareLock);
	/* Build scankey for every attribute in the index. */
	scankey = build_index_scankeys(in_rel,
								   idxrel,
								   searchslot,
								   NULL,
								   &num_scankeys,
								   SEGMENTBY_KEYS | NEXT_SEQNUM_KEYS);
	index_scan = index_beginscan(in_rel, idxrel, GetTransactionSnapshot(), num_scankeys, 0);
	index_rescan(index_scan, scankey, num_scankeys, NULL, 0);
	index_slot = table_slot_create(in_rel, NULL);
	ret = index_getnext_slot(index_scan, ForwardScanDirection, index_slot);
	ExecDropSingleTupleTableSlot(index_slot);
	index_endscan(index_scan);
	index_close(idxrel, AccessShareLock);
	return !ret;
}

/*
 * Helper method to handle all uncompressed rows which has
 * segmentby columns with new values.
 */
static void
process_new_segments(RowDecompressor *decompressor, Chunk *uncompressed_chunk,
					 Tuplesortstate *tuplestore, AffectedSegmentsCxt *affectedsegments,
					 int nsegmentby_cols)
{
	CompressedSegmentInfo **current_segment = NULL;
	TupleTableSlot *uncompressed_slot = NULL;
	unsigned int num_non_matching_compressedrows = 1;
	unsigned int new_segment_count = 0;
	uncompressed_slot = MakeTupleTableSlot(decompressor->out_desc, &TTSOpsMinimalTuple);

	while (tuplesort_gettupleslot(tuplestore,
								  true /*=forward*/,
								  false /*=copy*/,
								  uncompressed_slot,
								  NULL /*=abbrev*/))
	{
		slot_getallattrs(uncompressed_slot);
		if (!current_segment)
		{
			/* initialize segment */
			current_segment = palloc(sizeof(CompressedSegmentInfo *) * nsegmentby_cols);
			for (int i = 0; i < nsegmentby_cols; i++)
				current_segment[i] = palloc(sizeof(CompressedSegmentInfo));
			initialize_segment(decompressor,
							   current_segment,
							   uncompressed_chunk,
							   uncompressed_slot);
		}
		num_non_matching_compressedrows++;
		/* check if there is change in segmentby value */
		bool is_new_segment =
			is_segment_changed(current_segment, uncompressed_slot, nsegmentby_cols);

		if (is_new_segment)
		{
			/* update new segment */
			update_current_segment(current_segment, uncompressed_slot, nsegmentby_cols);
			if (affectedsegments->new_segments_found)
			{
				if (new_segment_count >= affectedsegments->new_segments_default_size)
					affectedsegments = resize_affectedsegmentscxt(affectedsegments);
				affectedsegments->total_new_compressed_segments++;
				affectedsegments->num_non_matching_compressedrows[new_segment_count++] =
					num_non_matching_compressedrows - 1;
				num_non_matching_compressedrows = 1;
			}
		}
	}
	ExecDropSingleTupleTableSlot(uncompressed_slot);
	/* last row present in tuplestore should be handled */
	affectedsegments->total_new_compressed_segments++;
	if (new_segment_count >= affectedsegments->new_segments_default_size)
		affectedsegments = resize_affectedsegmentscxt(affectedsegments);
	affectedsegments->num_non_matching_compressedrows[new_segment_count++] =
		num_non_matching_compressedrows;
	num_non_matching_compressedrows = 0;
}

/*
 * Helper method to fetch all affected compressed rows from compressed chunk
 * This method will do an indexscan on compressed chunk to fetch the required
 * compressed rows.
 */
static void
save_affected_compressed_rows_using_indexscan(RowDecompressor *decompressor,
											  RowCompressor *row_compressor,
											  Chunk *compressed_chunk, Chunk *uncompressed_chunk,
											  CompressTuplesortstateCxt *uncompressed_tuplestorecxt,
											  AffectedSegmentsCxt *affectedsegments,
											  int nsegmentby_cols)
{
	Relation index_rel;
	IndexScanDesc index_scan;
	HeapTuple compressed_tuple = NULL;
	HeapTuple uncompressed_tuple = NULL;
	TupleTableSlot *uncompressed_slot = NULL;
	TupleTableSlot *compressed_slot = NULL;
	ScanKeyData *segment_by_keys = NULL;
	ScanKeyData *segment_order_by_keys = NULL;

	int num_segkeys = 0;
	int num_scankeys = 0;
	unsigned int num_uncompressedrows = 0;
	unsigned int uncompressed_row_count = 0;

	bool compressed_tuple_found = false;
	bool segment_exists = false;

	/* Index scan */
	index_rel = index_open(row_compressor->index_oid, AccessShareLock);
	index_scan = index_beginscan(decompressor->in_rel,
								 index_rel,
								 GetTransactionSnapshot(),
								 nsegmentby_cols,
								 0);

	compressed_slot = table_slot_create(decompressor->in_rel, NULL);
	uncompressed_slot = MakeTupleTableSlot(decompressor->out_desc, &TTSOpsMinimalTuple);

	/*
	 * 1. For every uncompressed row do following:
	 *   A. If fetched compressed tuple (CR) from (iii) exists:
	 *       - If uncompressed row matching CR goto step1.
	 *       - If uncompressed row does not match, save total
	 *       - fetched uncompressed row count and CR in AffectedSegmentsCxt
	 *   B. Scan the compressed index
	 *       i. Fetch the compressed tuple based on segmentby values
	 *       ii. If matching segmentby tuple is found check the orderby values
	 *       iii. Save the matching segmentby/orderby compressed row (CR)
	 *       iv. If segmentby values does not match goto stepB
	 *       v. If none of compressed rows match the segmentby values mark
	 *          new_segments_found = true and process all new segmentby uncompressed
	 *          rows.
	 *   C. Handle all corner cases as part of steps A,B
	 */
	while (tuplesort_gettupleslot(uncompressed_tuplestorecxt->tuplestore,
								  true /*=forward*/,
								  false /*=copy*/,
								  uncompressed_slot,
								  NULL /*=abbrev*/))
	{
		Bitmapset *null_columns = NULL;
		ScanKeyData *idx_scankey = NULL;
		int num_idx_scankeys = 0;

		num_uncompressedrows++;
		slot_getallattrs(uncompressed_slot);

		if (segment_by_keys)
			pfree(segment_by_keys);
		if (segment_order_by_keys)
			pfree(segment_order_by_keys);
		/* build scan keys to do lookup into uncompressed chunks */
		segment_by_keys = build_segment_order_by_scankeys(decompressor,
														  uncompressed_chunk,
														  uncompressed_slot,
														  &null_columns,
														  &num_segkeys,
														  SEGMENTBY_KEYS);
		segment_order_by_keys = build_segment_order_by_scankeys(decompressor,
																uncompressed_chunk,
																uncompressed_slot,
																NULL,
																&num_scankeys,
																SEGMENTBY_KEYS | ORDERBY_KEYS);
		if (compressed_tuple_found)
		{
			if (null_columns)
			{
				/*
				 * we cannot use HeapKeyTest to check for NULLS, thus perform
				 * manual checks by calling has_all_nulls()
				 */
				uncompressed_tuple = heap_form_tuple(uncompressed_slot->tts_tupleDescriptor,
													 uncompressed_slot->tts_values,
													 uncompressed_slot->tts_isnull);
				if (has_all_nulls(decompressor, compressed_tuple, uncompressed_tuple, null_columns))
				{
					/* all nullable segmentby columns have matching NULL values */
					heap_freetuple(uncompressed_tuple);
					continue;
				}
				heap_freetuple(uncompressed_tuple);
			}
			/* reset the flags */
			segment_exists = false;
			compressed_tuple_found = false;
			if (num_segkeys)
			{
				HeapKeyTest(compressed_tuple,
							RelationGetDescr(decompressor->in_rel),
							num_segkeys,
							segment_by_keys,
							segment_exists);
				HeapKeyTest(compressed_tuple,
							RelationGetDescr(decompressor->in_rel),
							num_scankeys,
							segment_order_by_keys,
							compressed_tuple_found);
			}
			if (!compressed_tuple_found)
			{
				/*
				 * current uncompressed tuple did not fit into previously fetched
				 * compressed tuple.
				 * If current uncompressed tuple belongs to last batch in
				 * current segment, then append these uncompressed tuples, else
				 * new batch gets created assuming that these are new segments.
				 */
				if (!is_compressed_tuple_in_last_batch(decompressor->in_rel,
													   row_compressor->index_oid,
													   compressed_slot) ||
					!segment_exists)
				{
					if (uncompressed_row_count >= affectedsegments->default_size)
						affectedsegments = resize_affectedsegmentscxt(affectedsegments);
					affectedsegments->num_matching_uncompressedrows[uncompressed_row_count++] =
						num_uncompressedrows - 1;
					num_uncompressedrows = 1;
					affectedsegments
						->compressed_tuples[affectedsegments->total_affected_compressed_rows] =
						heap_copytuple(compressed_tuple);
					heap_freetuple(compressed_tuple);
					affectedsegments->total_affected_compressed_rows++;
				}
				else
				{
					compressed_tuple_found = true;
					continue;
				}
			}
			else
				continue;
		}
		else if (segment_exists)
		{
			/*
			 * In some cases, compressed tuples with current segmentby values exists
			 * however this tuples do not match the orderby values. ex:
			 * uncompressed rows:
			 * ('1999-12-30 15:30:00+05:30'::timestamp with time zone, 1, 2, 3, 22.2, 33.3);
			 * ('1999-12-31 05:30:00+05:30'::timestamp with time zone, 1, 2, 3, 2.2, 3.3);
			 * compressed rows:
			 *  1 |  10 |   1000 | Tue Jan 04 06:36:00 2000 PST | Wed Jan 05 15:54:00 2000 PST
			 *	1 |  20 |   1000 | Sun Jan 02 21:16:00 2000 PST | Tue Jan 04 06:34:00 2000 PST
			 *	1 |  30 |   1000 | Sat Jan 01 11:56:00 2000 PST | Sun Jan 02 21:14:00 2000 PST
			 *	1 |  40 |    598 | Fri Dec 31 16:00:00 1999 PST | Sat Jan 01 11:54:00 2000 PST
			 *
			 * For these kind of situations we need to append these uncompressed rows to
			 * compressed tuple with sequence num = 40
			 */

			HeapKeyTest(compressed_tuple,
						RelationGetDescr(decompressor->in_rel),
						num_segkeys,
						segment_by_keys,
						segment_exists);
			if (!segment_exists)
			{
				if (uncompressed_row_count >= affectedsegments->default_size)
					affectedsegments = resize_affectedsegmentscxt(affectedsegments);
				affectedsegments->num_matching_uncompressedrows[uncompressed_row_count++] =
					num_uncompressedrows - 1;
				num_uncompressedrows = 1;
				affectedsegments
					->compressed_tuples[affectedsegments->total_affected_compressed_rows] =
					heap_copytuple(compressed_tuple);
				heap_freetuple(compressed_tuple);
				affectedsegments->total_affected_compressed_rows++;
			}
			else
				continue;
		}
		idx_scankey = build_index_scankeys(decompressor->out_rel,
										   index_rel,
										   NULL,
										   uncompressed_slot,
										   &num_idx_scankeys,
										   SEGMENTBY_KEYS);
		index_rescan(index_scan, idx_scankey, num_idx_scankeys, NULL, 0);
		segment_exists = false;
		/*
		 * For every compressed tuple fetched which matches segmentby values:
		 *  - check if uncompressed tuple matches the compressed tuple
		 *  - If orderby matches, save this compressed tuple
		 *  - If only segmentby matches, move to next compressed tuple
		 *  - If no segmentby matches mark new_segments_found to true
		 */
		while (index_getnext_slot(index_scan, ForwardScanDirection, compressed_slot))
		{
			segment_exists = true;
			slot_getallattrs(compressed_slot);
			HeapTuple tuple = heap_form_tuple(compressed_slot->tts_tupleDescriptor,
											  compressed_slot->tts_values,
											  compressed_slot->tts_isnull);
			tuple->t_self = compressed_slot->tts_tid;
			compressed_tuple = heap_copytuple(tuple);
			heap_freetuple(tuple);
			compressed_tuple->t_self = compressed_slot->tts_tid;
			/* reset the flags */
			compressed_tuple_found = false;
			/*
			 * check if current uncompressed tuples orderby columns match the
			 * min/max meta columns from this compressed tuple. If matches set
			 * compressed_tuple_found to true.
			 */
			HeapKeyTest(compressed_tuple,
						RelationGetDescr(decompressor->in_rel),
						num_scankeys,
						segment_order_by_keys,
						compressed_tuple_found);
			/*
			 * Stop index scan for now and for this compressed tuple, find
			 * all matching uncompressed tuples and save this compressed tuple
			 * and total matching uncompressed tuple count in affectedsegments
			 */
			if (compressed_tuple_found)
				break;
		}
		if (!segment_exists)
		{
			/* new segment found */
			affectedsegments->new_segments_found = true;
			break;
		}
	}
	/* process all new segments uncompressed rows here */
	if (affectedsegments->new_segments_found)
	{
		process_new_segments(decompressor,
							 uncompressed_chunk,
							 uncompressed_tuplestorecxt->tuplestore,
							 affectedsegments,
							 nsegmentby_cols);
	}
	else
	{
		if (uncompressed_row_count >= affectedsegments->default_size)
			affectedsegments = resize_affectedsegmentscxt(affectedsegments);
		/* last row present in tuplestore should be handled */
		affectedsegments->num_matching_uncompressedrows[uncompressed_row_count++] =
			num_uncompressedrows;
		affectedsegments->compressed_tuples[affectedsegments->total_affected_compressed_rows] =
			heap_copytuple(compressed_tuple);
		heap_freetuple(compressed_tuple);
		affectedsegments->total_affected_compressed_rows++;
	}
	/* cleanup */
	ExecDropSingleTupleTableSlot(uncompressed_slot);
	ExecDropSingleTupleTableSlot(compressed_slot);
	index_endscan(index_scan);
	index_close(index_rel, AccessShareLock);
}

/*
 * Helper method to decompress a compressed tuple.
 * This method will return the sequence number of the current
 * compressed tuple.
 */
static int
decompress_in_tuplestore(RowDecompressor *decompressor, HeapTuple compressed_tuple,
						 Tuplesortstate *merge_tuplestore)
{
	Assert(HeapTupleIsValid(compressed_tuple));
	heap_deform_tuple(compressed_tuple,
					  decompressor->in_desc,
					  decompressor->compressed_datums,
					  decompressor->compressed_is_nulls);
	/* fetch sequence number of current compressed tuple */
	AttrNumber seq_attnum =
		get_attnum(decompressor->in_rel->rd_id, COMPRESSION_COLUMN_METADATA_SEQUENCE_NUM_NAME);
	/* decompress found tuple and delete from compressed chunk */
	row_decompressor_decompress_row(decompressor, merge_tuplestore);
	TM_FailureData tmfd;
	TM_Result result pg_attribute_unused();
	result = table_tuple_delete(decompressor->in_rel,
								&compressed_tuple->t_self,
								GetCurrentCommandId(true),
								GetTransactionSnapshot(),
								InvalidSnapshot,
								true,
								&tmfd,
								false);
	Assert(result == TM_Ok);
	/* make changes visible */
	CommandCounterIncrement();
	return decompressor->compressed_datums[seq_attnum - 1];
}

static void
recompress_affected_segments(RowDecompressor *decompressor, RowCompressor *row_compressor,
							 AffectedSegmentsCxt *affectedsegments, Chunk *uncompressed_chunk,
							 int nsegmentby_cols,
							 CompressTuplesortstateCxt *uncompressed_tuplestorecxt,
							 const ColumnCompressionInfo **keys, int n_keys)
{
	TupleTableSlot *compressed_slot = NULL;
	TupleTableSlot *uncompressed_slot = NULL;
	HeapTuple next_tuple = NULL;
	unsigned int total_uncompressed_rows_per_segment = 0;
	unsigned int compressed_row_count = 0;
	Datum metacount_val;
	AttrNumber meta_count;

	compressed_slot = MakeTupleTableSlot(decompressor->in_desc, &TTSOpsHeapTuple);
	while (affectedsegments->total_affected_compressed_rows)
	{
		ExecStoreHeapTuple(affectedsegments->compressed_tuples[compressed_row_count],
						   compressed_slot,
						   false);
		heap_deform_tuple(affectedsegments->compressed_tuples[compressed_row_count],
						  decompressor->in_desc,
						  compressed_slot->tts_values,
						  compressed_slot->tts_isnull);
		slot_getallattrs(compressed_slot);
		meta_count =
			get_attnum(decompressor->in_rel->rd_id, COMPRESSION_COLUMN_METADATA_COUNT_NAME);
		metacount_val = compressed_slot->tts_values[meta_count - 1];

		/*
		 * Calculate number of compressed batches to decompress in case
		 * there is an overflow:
		 * For a given compressed batch with sequence# 30, if there is an overflow,
		 * then we save rest of rows > 1000 in next sequence# which is 31. However
		 * if total_uncompressed_rows_per_segment is > (10 * MAX_ROWS_PER_COMPRESSION)
		 * then we run out of sequence numbers from (31 .. 39) thus we
		 * need to decompress the next segment and repeat the same until we
		 * see that all uncompressed rows fits in.
		 *
		 * (total_uncompressed_rows_per_segment + meta_count) / (10 * MAX_ROWS_PER_COMPRESSION)
		 * should give us, how many following compressed batches to decompress.
		 *
		 * Based on the calculated value, we fetch next compressed batches and perform
		 * recompression.
		 */
		total_uncompressed_rows_per_segment =
			affectedsegments->num_matching_uncompressedrows[compressed_row_count];
		unsigned int total_batches_to_decompress =
			((total_uncompressed_rows_per_segment + metacount_val) /
			 (10 * MAX_ROWS_PER_COMPRESSION));

		CompressTuplesortstateCxt *tuplestorecxt = NULL;
		tuplestorecxt = initialize_tuplestore(decompressor->out_rel, keys, n_keys);
		uncompressed_slot = MakeTupleTableSlot(decompressor->out_desc, &TTSOpsMinimalTuple);
		while (affectedsegments->num_matching_uncompressedrows[compressed_row_count] &&
			   tuplesort_gettupleslot(uncompressed_tuplestorecxt->tuplestore,
									  true /*=forward*/,
									  false /*=copy*/,
									  uncompressed_slot,
									  NULL /*=abbrev*/))
		{
			tuplesort_puttupleslot(tuplestorecxt->tuplestore, uncompressed_slot);
			affectedsegments->num_matching_uncompressedrows[compressed_row_count]--;
		}
		ExecDropSingleTupleTableSlot(uncompressed_slot);
		if (next_tuple)
		{
			/* this compressed tuple might have already been decompressed */
			ItemPointer ptr1 = &(next_tuple)->t_self;
			ItemPointer ptr2 = &affectedsegments->compressed_tuples[compressed_row_count]->t_self;
			if (ItemPointerCompare(ptr1, ptr2) != 0)
			{
				/* decompress affected batch */
				row_compressor->sequence_num =
					decompress_in_tuplestore(decompressor,
											 affectedsegments
												 ->compressed_tuples[compressed_row_count],
											 tuplestorecxt->tuplestore);
			}
			else
			{
				heap_freetuple(next_tuple);
				next_tuple = NULL;
			}
		}
		else
		{
			/* decompress affected batch */
			row_compressor->sequence_num =
				decompress_in_tuplestore(decompressor,
										 affectedsegments->compressed_tuples[compressed_row_count],
										 tuplestorecxt->tuplestore);
		}
		while (total_batches_to_decompress)
		{
			/* fetch next batches which will be affected */
			if (fetch_next_compressed_tuple(decompressor,
											row_compressor,
											uncompressed_chunk,
											nsegmentby_cols,
											compressed_slot,
											&next_tuple))
			{
				/* decompress affected batch */
				decompress_in_tuplestore(decompressor, next_tuple, tuplestorecxt->tuplestore);
			}
			total_batches_to_decompress--;
		}
		tuplesort_performsort(tuplestorecxt->tuplestore);
		recompress_segment(tuplestorecxt->tuplestore, decompressor->out_rel, row_compressor);
		cleanup_tuplestorecxt(tuplestorecxt);
		compressed_row_count++;
		affectedsegments->total_affected_compressed_rows--;
	}
	ExecDropSingleTupleTableSlot(compressed_slot);
}

static void
do_legacy_recompression(RowDecompressor *decompressor, RowCompressor *row_compressor,
						CompressTuplesortstateCxt *uncompressed_tuplestorecxt)
{
	Relation idxrel;
	IndexScanDesc index_scan;
	TupleTableSlot *compressed_slot = NULL;

	idxrel = index_open(row_compressor->index_oid, AccessShareLock);
	index_scan = index_beginscan(decompressor->in_rel, idxrel, GetTransactionSnapshot(), 0, 0);
	index_rescan(index_scan, NULL, 0, NULL, 0);
	compressed_slot = table_slot_create(decompressor->in_rel, NULL);

	while (index_getnext_slot(index_scan, ForwardScanDirection, compressed_slot))
	{
		slot_getallattrs(compressed_slot);
		HeapTuple tuple = heap_form_tuple(compressed_slot->tts_tupleDescriptor,
										  compressed_slot->tts_values,
										  compressed_slot->tts_isnull);
		tuple->t_self = compressed_slot->tts_tid;
		decompress_in_tuplestore(decompressor, tuple, uncompressed_tuplestorecxt->tuplestore);
		heap_freetuple(tuple);
	}
	ExecDropSingleTupleTableSlot(compressed_slot);
	index_endscan(index_scan);
	index_close(idxrel, AccessShareLock);

	tuplesort_performsort(uncompressed_tuplestorecxt->tuplestore);
	recompress_segment(uncompressed_tuplestorecxt->tuplestore,
					   decompressor->out_rel,
					   row_compressor);
}

static void
recompress_new_segments(RowDecompressor *decompressor, RowCompressor *row_compressor,
						AffectedSegmentsCxt *affectedsegments,
						CompressTuplesortstateCxt *uncompressed_tuplestorecxt,
						const ColumnCompressionInfo **keys, int n_keys)
{
	TupleTableSlot *uncompressed_slot;
	int count = 0;
	uncompressed_slot = MakeTupleTableSlot(decompressor->out_desc, &TTSOpsMinimalTuple);
	while (affectedsegments->total_new_compressed_segments)
	{
		CompressTuplesortstateCxt *tuplestorecxt;
		tuplestorecxt = initialize_tuplestore(decompressor->out_rel, keys, n_keys);

		while (affectedsegments->num_non_matching_compressedrows[count] &&
			   tuplesort_gettupleslot(uncompressed_tuplestorecxt->tuplestore,
									  true /*=forward*/,
									  false /*=copy*/,
									  uncompressed_slot,
									  NULL /*=abbrev*/))
		{
			tuplesort_puttupleslot(tuplestorecxt->tuplestore, uncompressed_slot);
			affectedsegments->num_non_matching_compressedrows[count]--;
		}
		tuplesort_performsort(tuplestorecxt->tuplestore);
		recompress_segment(tuplestorecxt->tuplestore, decompressor->out_rel, row_compressor);
		cleanup_tuplestorecxt(tuplestorecxt);
		affectedsegments->total_new_compressed_segments--;
		count++;
	}
	ExecDropSingleTupleTableSlot(uncompressed_slot);
}

static void
recompress_partial_chunks(Chunk *uncompressed_chunk, Chunk *compressed_chunk)
{
	Relation uncompressed_chunk_rel;
	Relation compressed_chunk_rel;
	RowDecompressor decompressor;
	RowCompressor row_compressor;
	const ColumnCompressionInfo **colinfo_array = NULL;
	const ColumnCompressionInfo **keys = NULL;
	CompressTuplesortstateCxt *uncompressed_tuplestorecxt;
	List *htcols_list;
	ListCell *lc;

	int htcols_listlen = 0;
	int nsegmentby_cols = 0;
	int i = 0;
	unsigned long total_segments_decompressed = 0;
	unsigned long total_new_segments = 0;
	unsigned long total_uncompressed_rows_per_chunk = 0;

	int n_keys = 0;
	int16 *in_column_offsets = NULL;

	htcols_list = ts_hypertable_compression_get(uncompressed_chunk->fd.hypertable_id);
	htcols_listlen = list_length(htcols_list);

	colinfo_array = palloc(sizeof(ColumnCompressionInfo *) * htcols_listlen);
	foreach (lc, htcols_list)
	{
		FormData_hypertable_compression *fd = (FormData_hypertable_compression *) lfirst(lc);
		colinfo_array[i++] = fd;
		if (COMPRESSIONCOL_IS_SEGMENT_BY(fd))
			nsegmentby_cols++;
	}
	in_column_offsets = compress_chunk_populate_keys(uncompressed_chunk->table_id,
													 colinfo_array,
													 htcols_listlen,
													 &n_keys,
													 &keys);

	uncompressed_chunk_rel = RelationIdGetRelation(uncompressed_chunk->table_id);
	compressed_chunk_rel = RelationIdGetRelation(compressed_chunk->table_id);
	decompressor = build_decompressor(compressed_chunk_rel, uncompressed_chunk_rel);
	/* do not need the indexes on the uncompressed chunk as we do not write to it anymore */
	ts_catalog_close_indexes(decompressor.indexstate);

	row_compressor_init(&row_compressor,
						RelationGetDescr(uncompressed_chunk_rel),
						compressed_chunk_rel,
						htcols_listlen,
						colinfo_array,
						in_column_offsets,
						RelationGetDescr(compressed_chunk_rel)->natts,
						true /*need_bistate*/,
						true /*reset_sequence*/,
						RECOMPRESS);

	uncompressed_tuplestorecxt = initialize_tuplestore(uncompressed_chunk_rel, keys, n_keys);
	/* save all rows from uncompressed chunk into tuplestore */
	total_uncompressed_rows_per_chunk =
		save_uncompressed_rows_in_tuplestore(uncompressed_chunk,
											 uncompressed_tuplestorecxt->tuplestore);

	if (total_uncompressed_rows_per_chunk)
	{
		CompressedChunkStats compressedstats = {
			.num_rows_pre_compression = 0,
			.num_rows_post_compression = 0,
			.num_expected_gaps_in_sequences = 0,
			.num_gaps_in_sequences = 0,
			.num_segments = 0,
			.num_batches_per_segments = { 0 },
		};

		AffectedSegmentsCxt affectedsegments = {
			.compressed_tuples = palloc0(sizeof(unsigned long) * MAX_ROWS_PER_COMPRESSION),
			.num_matching_uncompressedrows =
				palloc0(sizeof(unsigned long) * MAX_ROWS_PER_COMPRESSION),
			.total_affected_compressed_rows = 0,
			.new_segments_found = false,
			.num_non_matching_compressedrows =
				palloc0(sizeof(unsigned long) * MAX_ROWS_PER_COMPRESSION),
			.total_new_compressed_segments = 0,
			.default_size = MAX_ROWS_PER_COMPRESSION,
			.new_segments_default_size = MAX_ROWS_PER_COMPRESSION,
		};
		if (!OidIsValid(row_compressor.index_oid))
			ereport(ERROR, (errcode(ERRCODE_UNDEFINED_TABLE), errmsg("index does not exist")));

		/* fetch initial statistics of current chunk */
		get_compressed_chunk_statistics(decompressor.in_rel,
										row_compressor.index_oid,
										&compressedstats);
		/*
		 * If there is 30% gap available in compressed chunk, recompress
		 * only affected segments, else call legacy method of recompression
		 * (decompress + compress)
		 */
		if ((compressedstats.num_expected_gaps_in_sequences * 0.3) <=
			compressedstats.num_gaps_in_sequences)
		{
			save_affected_compressed_rows_using_indexscan(&decompressor,
														  &row_compressor,
														  compressed_chunk,
														  uncompressed_chunk,
														  uncompressed_tuplestorecxt,
														  &affectedsegments,
														  nsegmentby_cols);

			/* mark current scan position to starting point */
			tuplesort_rescan(uncompressed_tuplestorecxt->tuplestore);
			total_segments_decompressed = affectedsegments.total_affected_compressed_rows;
			if (affectedsegments.total_affected_compressed_rows)
			{
				row_compressor.update_sequence = false;
				recompress_affected_segments(&decompressor,
											 &row_compressor,
											 &affectedsegments,
											 uncompressed_chunk,
											 nsegmentby_cols,
											 uncompressed_tuplestorecxt,
											 keys,
											 n_keys);
			}
			total_new_segments = affectedsegments.total_new_compressed_segments;
			if (affectedsegments.total_new_compressed_segments)
			{
				row_compressor.update_sequence = true;
				row_compressor.reset_sequence = true;
				recompress_new_segments(&decompressor,
										&row_compressor,
										&affectedsegments,
										uncompressed_tuplestorecxt,
										keys,
										n_keys);
			}
		}
		else
		{
			total_segments_decompressed = compressedstats.num_rows_post_compression;
			row_compressor.compression_type = COMPRESS;
			do_legacy_recompression(&decompressor, &row_compressor, uncompressed_tuplestorecxt);
		}
		/****** update compression statistics ******/
		RelationSize after_size = ts_relation_size_impl(compressed_chunk->table_id);
		/* the compression size statistics we are able to update and accurately report are:
		 * rowcount pre/post compression,
		 * compressed chunk sizes */
		row_compressor.rowcnt_pre_compression =
			compressedstats.num_rows_pre_compression + total_uncompressed_rows_per_chunk;
		row_compressor.num_compressed_rows =
			compressedstats.num_rows_post_compression + total_new_segments;
		compression_chunk_size_catalog_update_recompressed(uncompressed_chunk->fd.id,
														   compressed_chunk->fd.id,
														   &after_size,
														   row_compressor.rowcnt_pre_compression,
														   row_compressor.num_compressed_rows);
		elog(LOG, "Total decompressed segments are: %lu", total_segments_decompressed);
		/* freeup AffectedSegmentsCxt */
		pfree(affectedsegments.compressed_tuples);
		pfree(affectedsegments.num_matching_uncompressedrows);
		pfree(affectedsegments.num_non_matching_compressedrows);
	}
	cleanup_tuplestorecxt(uncompressed_tuplestorecxt);
	row_compressor_finish(&row_compressor);
	FreeBulkInsertState(decompressor.bistate);

	RelationClose(uncompressed_chunk_rel);
	RelationClose(compressed_chunk_rel);
}
