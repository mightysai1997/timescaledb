/*
 * This file and its contents are licensed under the Timescale License.
 * Please see the included NOTICE for copyright information and
 * LICENSE-TIMESCALE for a copy of the license.
 */

#include <postgres.h>
#include <access/valid.h>
#include <access/xact.h>
#include <utils/snapmgr.h>
#include <utils/syscache.h>
#include <utils/typcache.h>

#include "recompression.h"
#include "compression.h"
#include "guc.h"
#include "hypertable_cache.h"
#include "ts_catalog/hypertable_compression.h"
#include "create.h"

typedef struct CompressTuplesortstateCxt
{
	Tuplesortstate *tuplestore;
	AttrNumber *sort_keys;
	Oid *sort_operators;
	Oid *sort_collations;
	bool *nulls_first;
} CompressTuplesortstateCxt;

/*
 * Placeholder to save all context related to compressed chunk
 */
typedef struct CompressedChunkStats
{
	/* total no: of uncompressed rows */
	unsigned long num_rows_pre_compression;
	/* total no: of compressed rows */
	unsigned long num_rows_post_compression;
	/* total count of gaps between compressed rows */
	unsigned long num_gaps_in_sequences;
	/* total count of gaps between compressed rows generated by compress_chunk() API */
	unsigned long num_expected_gaps_in_sequences;
	/* total unique segmentby column values */
	unsigned long num_segments;
	/* total number of compressed batches per unique segmentby column values */
	unsigned long num_batches_per_segments[MAX_ROWS_PER_COMPRESSION];
} CompressedChunkStats;

/*
 * Placeholder to save all context related to recompression of partial chunks
 */
typedef struct AffectedSegmentsCxt
{
	/* holds matching compressed tuples */
	HeapTuple *compressed_tuples;
	/* holds total number of uncompressed tuples which match the compressed tuple */
	unsigned long *num_matching_uncompressedrows;
	/* holds total affected compressed segments */
	unsigned long total_affected_compressed_rows;
	/* flag to know if new segments are found in uncompressed chunk */
	bool new_segments_found;
	/* holds total number of uncompressed tuples with same segmentby column values */
	unsigned long *num_non_matching_compressedrows;
	/* holds total affected new segmentby column values */
	unsigned long total_new_compressed_segments;
	/* default size of above arrays */
	unsigned long default_size;
	/* default size of above arrays */
	unsigned long new_segments_default_size;
} AffectedSegmentsCxt;

/*
 * Helper method to resize AffectedSegmentsCxt when total affected
 * compressed tuples exceeds its default value of MAX_ROWS_PER_COMPRESSION
 * This method will double the size of arrays present in AffectedSegmentsCxt
 * and moves contents of existing AffectedSegmentsCxt to new context.
 */
static AffectedSegmentsCxt *
resize_affectedsegmentscxt(AffectedSegmentsCxt *as)
{
	AffectedSegmentsCxt *new_cxt = (AffectedSegmentsCxt *) palloc0(sizeof(AffectedSegmentsCxt));
	unsigned long factor = 0;
	unsigned long new_size = 0;

	if (!as->new_segments_found)
	{
		factor = (as->default_size / MAX_ROWS_PER_COMPRESSION) + 1;
		new_size = factor * MAX_ROWS_PER_COMPRESSION;
		/* copy only affected compressed segments context */
		new_cxt->num_matching_uncompressedrows =
			repalloc(as->num_matching_uncompressedrows, sizeof(unsigned long) * new_size);
		new_cxt->compressed_tuples =
			repalloc(as->compressed_tuples, sizeof(unsigned long) * new_size);

		new_cxt->num_non_matching_compressedrows = as->num_non_matching_compressedrows;
		new_cxt->default_size = new_size;
		new_cxt->new_segments_default_size = as->new_segments_default_size;
	}
	else
	{
		factor = (as->new_segments_default_size / MAX_ROWS_PER_COMPRESSION) + 1;
		new_size = factor * MAX_ROWS_PER_COMPRESSION;
		/* copy all new compressed segments context */
		new_cxt->num_non_matching_compressedrows =
			repalloc(as->num_non_matching_compressedrows, sizeof(unsigned long) * new_size);

		new_cxt->num_matching_uncompressedrows = as->num_matching_uncompressedrows;
		new_cxt->compressed_tuples = as->compressed_tuples;
		new_cxt->new_segments_default_size = new_size;
		new_cxt->default_size = as->default_size;
	}
	/* copy rest of the contents */
	new_cxt->total_affected_compressed_rows = as->total_affected_compressed_rows;
	new_cxt->new_segments_found = as->new_segments_found;
	new_cxt->total_new_compressed_segments = as->total_new_compressed_segments;
	pfree(as);
	return new_cxt;
}

/* Helper method to update current segment which is being recompressed */
static void
update_current_segment(CompressedSegmentInfo **current_segment, TupleTableSlot *slot,
					   const int nsegmentby_cols)
{
	Datum val;
	bool is_null;
	int seg_idx = 0;
	for (int i = 0; i < nsegmentby_cols; i++)
	{
		int16 col_offset = current_segment[i]->decompressed_chunk_offset;
		val = slot_getattr(slot, AttrOffsetGetAttrNumber(col_offset), &is_null);
		if (!segment_info_datum_is_in_group(current_segment[seg_idx++]->segment_info, val, is_null))
		{
			/* new segment, need to do per-segment processing */
			pfree(current_segment[seg_idx - 1]->segment_info); /* because increased previously */
			SegmentInfo *segment_info =
				segment_info_new(TupleDescAttr(slot->tts_tupleDescriptor, col_offset));
			segment_info_update(segment_info, val, is_null);
			current_segment[seg_idx - 1]->segment_info = segment_info;
			current_segment[seg_idx - 1]->decompressed_chunk_offset = col_offset;
		}
	}
}

/* Helper method to find if segment being recompressed, has encountered a new segment */
static bool
is_segment_changed(CompressedSegmentInfo **current_segment, TupleTableSlot *slot,
				   const int nsegmentby_cols)
{
	Datum val;
	bool is_null;
	bool changed_segment = false;
	for (int i = 0; i < nsegmentby_cols; i++)
	{
		int16 col_offset = current_segment[i]->decompressed_chunk_offset;
		val = slot_getattr(slot, AttrOffsetGetAttrNumber(col_offset), &is_null);
		if (!segment_info_datum_is_in_group(current_segment[i++]->segment_info, val, is_null))
		{
			changed_segment = true;
			break;
		}
	}
	return changed_segment;
}

/* This is a wrapper around row_compressor_append_sorted_rows. */
static void
recompress_segment(Tuplesortstate *tuplesortstate, const Relation compressed_chunk_rel,
				   RowCompressor *row_compressor)
{
	row_compressor_append_sorted_rows(row_compressor,
									  tuplesortstate,
									  RelationGetDescr(compressed_chunk_rel));
	/* make changes visible */
	CommandCounterIncrement();
}

/*
 * This function updates catalog chunk compression statistics
 * for an existing compressed chunk after it has been recompressed
 * segmentwise in-place (as opposed to creating a new compressed chunk).
 * Note that because of this it is not possible to accurately report
 * the fields
 * uncompressed_chunk_size, uncompressed_index_size, uncompressed_toast_size
 * anymore, so these are not updated.
 */
static int
compression_chunk_size_catalog_update_recompressed(int32 uncompressed_chunk_id,
												   int32 compressed_chunk_id,
												   const RelationSize *recompressed_size,
												   int64 rowcnt_pre_compression,
												   int64 rowcnt_post_compression)
{
	ScanIterator iterator =
		ts_scan_iterator_create(COMPRESSION_CHUNK_SIZE, RowExclusiveLock, CurrentMemoryContext);
	bool updated = false;

	iterator.ctx.index =
		catalog_get_index(ts_catalog_get(), COMPRESSION_CHUNK_SIZE, COMPRESSION_CHUNK_SIZE_PKEY);
	ts_scan_iterator_scan_key_init(&iterator,
								   Anum_compression_chunk_size_pkey_chunk_id,
								   BTEqualStrategyNumber,
								   F_INT4EQ,
								   Int32GetDatum(uncompressed_chunk_id));
	ts_scanner_foreach(&iterator)
	{
		Datum values[Natts_compression_chunk_size];
		bool replIsnull[Natts_compression_chunk_size] = { false };
		bool repl[Natts_compression_chunk_size] = { false };
		bool should_free;
		TupleInfo *ti = ts_scan_iterator_tuple_info(&iterator);
		HeapTuple tuple = ts_scanner_fetch_heap_tuple(ti, false, &should_free);
		HeapTuple new_tuple = NULL;
		heap_deform_tuple(tuple, ts_scanner_get_tupledesc(ti), values, replIsnull);

		/* Only update the information pertaining to the compressed chunk */
		/* these fields are about the compressed chunk so they can be updated */
		values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_compressed_heap_size)] =
			Int64GetDatum(recompressed_size->heap_size);
		repl[AttrNumberGetAttrOffset(Anum_compression_chunk_size_compressed_heap_size)] = true;

		values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_compressed_toast_size)] =
			Int64GetDatum(recompressed_size->toast_size);
		repl[AttrNumberGetAttrOffset(Anum_compression_chunk_size_compressed_toast_size)] = true;

		values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_compressed_index_size)] =
			Int64GetDatum(recompressed_size->index_size);
		repl[AttrNumberGetAttrOffset(Anum_compression_chunk_size_compressed_index_size)] = true;

		values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_numrows_pre_compression)] =
			Int64GetDatum(rowcnt_pre_compression);
		repl[AttrNumberGetAttrOffset(Anum_compression_chunk_size_numrows_pre_compression)] = true;

		values[AttrNumberGetAttrOffset(Anum_compression_chunk_size_numrows_post_compression)] =
			Int64GetDatum(rowcnt_post_compression);
		repl[AttrNumberGetAttrOffset(Anum_compression_chunk_size_numrows_post_compression)] = true;

		new_tuple =
			heap_modify_tuple(tuple, ts_scanner_get_tupledesc(ti), values, replIsnull, repl);
		ts_catalog_update(ti->scanrel, new_tuple);
		heap_freetuple(new_tuple);

		if (should_free)
			heap_freetuple(tuple);

		updated = true;
		break;
	}

	ts_scan_iterator_end(&iterator);
	ts_scan_iterator_close(&iterator);
	return updated;
}

/*
 * Helper method to get statistics of compressed chunk, before recompression.
 */
static void
get_compressed_chunk_statistics(const Relation in_rel, const Oid index_oid,
								CompressedChunkStats *stats)
{
	Relation idxrel;
	IndexScanDesc index_scan;
	TupleTableSlot *heap_tuple_slot = NULL;
	AttrNumber metacount_attnum;
	AttrNumber seq_attnum;
	Datum curr_seqnum_val = SEQUENCE_NUM_GAP;
	Datum prev_seqnum_val = SEQUENCE_NUM_GAP;

	unsigned long total_batches_per_segment = 0;
	unsigned long total_segments = 0;

	idxrel = index_open(index_oid, AccessShareLock);
	index_scan = index_beginscan(in_rel, idxrel, GetTransactionSnapshot(), 0, 0);
	index_rescan(index_scan, NULL, 0, NULL, 0);
	heap_tuple_slot = table_slot_create(in_rel, NULL);
	metacount_attnum = get_attnum(in_rel->rd_id, COMPRESSION_COLUMN_METADATA_COUNT_NAME);
	seq_attnum = get_attnum(in_rel->rd_id, COMPRESSION_COLUMN_METADATA_SEQUENCE_NUM_NAME);

	while (index_getnext_slot(index_scan, ForwardScanDirection, heap_tuple_slot))
	{
		slot_getallattrs(heap_tuple_slot);
		stats->num_rows_pre_compression += heap_tuple_slot->tts_values[metacount_attnum - 1];
		stats->num_rows_post_compression++;
		curr_seqnum_val = heap_tuple_slot->tts_values[seq_attnum - 1];
		if (curr_seqnum_val > SEQUENCE_NUM_GAP)
		{
			stats->num_gaps_in_sequences += ((curr_seqnum_val - prev_seqnum_val) - 1);
			stats->num_expected_gaps_in_sequences += 9;
		}

		if (prev_seqnum_val > curr_seqnum_val)
		{
			/* there was change in segments */
			prev_seqnum_val = curr_seqnum_val = SEQUENCE_NUM_GAP;
			stats->num_batches_per_segments[total_segments] = total_batches_per_segment;
			total_segments++;
			total_batches_per_segment = 1;
		}
		else
		{
			total_batches_per_segment++;
			prev_seqnum_val = curr_seqnum_val;
		}
	}
	index_endscan(index_scan);
	index_close(idxrel, AccessShareLock);
	ExecDropSingleTupleTableSlot(heap_tuple_slot);

	stats->num_segments = total_segments + 1;
	stats->num_batches_per_segments[total_segments] = total_batches_per_segment;
}

/* Helper method to initial the first segment by column values in current_segment */
static void
initialize_segment(const RowDecompressor *decompressor, CompressedSegmentInfo **current_segment,
				   const Chunk *uncompressed_chunk, TupleTableSlot *uncompressed_slot)
{
	int i = 0;
	Datum val;
	bool is_null;
	SegmentInfo *segment_info = NULL;
	/* initialize current segment */
	for (int col = 0; col < uncompressed_slot->tts_tupleDescriptor->natts; col++)
	{
		if (uncompressed_slot->tts_tupleDescriptor->attrs[col].attisdropped)
			continue;
		val = slot_getattr(uncompressed_slot, AttrOffsetGetAttrNumber(col), &is_null);
		Form_pg_attribute decompressed_attr = TupleDescAttr(decompressor->out_desc, col);
		char *col_name = NameStr(decompressed_attr->attname);
		FormData_hypertable_compression *fd =
			ts_hypertable_compression_get_by_pkey(uncompressed_chunk->fd.hypertable_id, col_name);
		if (COMPRESSIONCOL_IS_SEGMENT_BY(fd))
		{
			segment_info =
				segment_info_new(TupleDescAttr(uncompressed_slot->tts_tupleDescriptor, col));
			current_segment[i]->decompressed_chunk_offset = col;
			/* also need to call segment_info_update here to update the val part */
			segment_info_update(segment_info, val, is_null);
			current_segment[i]->segment_info = segment_info;
			i++;
		}
	}
}

/* initialize tuplesort state */
static CompressTuplesortstateCxt *
initialize_tuplestore(const Relation uncompressed_chunk_rel, const ColumnCompressionInfo **keys,
					  const int n_keys)
{
	CompressTuplesortstateCxt *tuplestorecxt = NULL;
	int tuplesortopts = 0;
#if PG15_GE
	tuplesortopts = TUPLESORT_NONE | TUPLESORT_RANDOMACCESS;
#else
	tuplesortopts = 1;
#endif
	tuplestorecxt = (CompressTuplesortstateCxt *) palloc0(sizeof(CompressTuplesortstateCxt));
	tuplestorecxt->sort_keys = palloc(sizeof(AttrNumber) * n_keys);
	tuplestorecxt->sort_operators = palloc(sizeof(Oid) * n_keys);
	tuplestorecxt->sort_collations = palloc(sizeof(Oid) * n_keys);
	tuplestorecxt->nulls_first = palloc(sizeof(bool) * n_keys);

	for (int n = 0; n < n_keys; n++)
		compress_chunk_populate_sort_info_for_column(RelationGetRelid(uncompressed_chunk_rel),
													 keys[n],
													 &tuplestorecxt->sort_keys[n],
													 &tuplestorecxt->sort_operators[n],
													 &tuplestorecxt->sort_collations[n],
													 &tuplestorecxt->nulls_first[n]);

	tuplestorecxt->tuplestore = tuplesort_begin_heap(RelationGetDescr(uncompressed_chunk_rel),
													 n_keys,
													 tuplestorecxt->sort_keys,
													 tuplestorecxt->sort_operators,
													 tuplestorecxt->sort_collations,
													 tuplestorecxt->nulls_first,
													 maintenance_work_mem,
													 NULL,
													 tuplesortopts);
	return tuplestorecxt;
}

/* cleaup tuplesort state */
static void
cleanup_tuplestorecxt(CompressTuplesortstateCxt *tuplestorecxt)
{
	if (tuplestorecxt)
	{
		tuplesort_end(tuplestorecxt->tuplestore);
		pfree(tuplestorecxt->sort_keys);
		pfree(tuplestorecxt->sort_operators);
		pfree(tuplestorecxt->sort_collations);
		pfree(tuplestorecxt->nulls_first);
		pfree(tuplestorecxt);
		tuplestorecxt = NULL;
	}
}

/*
 * Helper method to fetch all rows from uncompressed chunk
 */
static int
save_uncompressed_rows_in_tuplestore(const Relation uncompressed_chunk_rel,
									 Tuplesortstate *uncompressed_rows_sortstate)
{
	TableScanDesc heapScan;
	TupleTableSlot *heap_tuple_slot = NULL;
	int total_uncompressed_rows = 0;

	heapScan = table_beginscan(uncompressed_chunk_rel, GetLatestSnapshot(), 0, NULL);
	heap_tuple_slot = table_slot_create(uncompressed_chunk_rel, NULL);

	while (table_scan_getnextslot(heapScan, ForwardScanDirection, heap_tuple_slot))
	{
		slot_getallattrs(heap_tuple_slot);
		tuplesort_puttupleslot(uncompressed_rows_sortstate, heap_tuple_slot);
		total_uncompressed_rows++;
	}
	ExecDropSingleTupleTableSlot(heap_tuple_slot);
	table_endscan(heapScan);
	return total_uncompressed_rows;
}

/*
 * Helper method used to build scankeys based on segmentby and orderby
 * key columns to do lookup into uncompressed chunks.
 */
static ScanKeyData *
build_segment_order_by_scankeys(const RowDecompressor *decompressor,
								const Chunk *uncompressed_chunk, TupleTableSlot *slot,
								Bitmapset **null_columns_count, int *num_scankeys,
								const int key_flags)
{
	ScanKeyData *segment_order_by_keys;
	Bitmapset *key_columns = NULL;
	Bitmapset *null_columns = NULL;
	int num_keys = 0;
	for (int i = 0; i < decompressor->out_desc->natts; i++)
	{
		Form_pg_attribute attr = TupleDescAttr(decompressor->out_desc, i);
		if (attr->attisdropped)
			continue;
		FormData_hypertable_compression *fd =
			ts_hypertable_compression_get_by_pkey(uncompressed_chunk->fd.hypertable_id,
												  attr->attname.data);
		if (key_flags & SEGMENTBY_KEYS)
		{
			if (COMPRESSIONCOL_IS_SEGMENT_BY(fd))
				key_columns =
					bms_add_member(key_columns, attr->attnum - FirstLowInvalidHeapAttributeNumber);
		}
		if (key_flags & ORDERBY_KEYS)
		{
			if (COMPRESSIONCOL_IS_ORDER_BY(fd))
				key_columns =
					bms_add_member(key_columns, attr->attnum - FirstLowInvalidHeapAttributeNumber);
		}
	}
	segment_order_by_keys = build_scankeys(uncompressed_chunk->fd.hypertable_id,
										   uncompressed_chunk->table_id,
										   *decompressor,
										   key_columns,
										   &null_columns,
										   slot,
										   &num_keys);
	if (null_columns_count)
		*null_columns_count = null_columns;
	bms_free(key_columns);
	*num_scankeys = num_keys;
	return segment_order_by_keys;
}

/*
 * Helper method to check if segmentby columns in compressed/uncompressed
 * tuples have NULL or not.
 *
 * Return true if all nullable segmentby columns have NULL values, else
 * return false.
 */
static bool
has_all_nulls(const RowDecompressor *decompressor, const HeapTuple compressed_tuple,
			  const HeapTuple uncompressed_tuple, const Bitmapset *null_columns)
{
	bool nulls_found = true;
	for (int attno = bms_next_member(null_columns, -1); attno >= 0;
		 attno = bms_next_member(null_columns, attno))
	{
		if (!(heap_attisnull(compressed_tuple, attno, decompressor->in_desc)))
		{
			nulls_found = false;
			break;
		}
		if (!(heap_attisnull(uncompressed_tuple, attno, decompressor->out_desc)))
		{
			nulls_found = false;
			break;
		}
	}
	return nulls_found;
}

/*
 * Helper method to build scankeys to do lookup into compressed chunk index
 */
static ScanKeyData *
build_index_scankeys(const Relation in_rel, const Relation idxrel,
					 const TupleTableSlot *compressedslot, const TupleTableSlot *uncompressedslot,
					 int *num_keys, const int key_flags)
{
	ScanKeyData *scankey = NULL;
	int indnkeyatts;
	int total_keys = 0;
	int attoff;
	bool isnull;
	Datum indclassDatum;
	oidvector *opclass;
	int2vector *indkey = NULL;

	indkey = &idxrel->rd_index->indkey;
	indclassDatum =
		SysCacheGetAttr(INDEXRELID, idxrel->rd_indextuple, Anum_pg_index_indclass, &isnull);
	Assert(!isnull);
	opclass = (oidvector *) DatumGetPointer(indclassDatum);

	indnkeyatts = IndexRelationGetNumberOfKeyAttributes(idxrel);
	scankey = palloc0(sizeof(ScanKeyData) * indnkeyatts);

	/* Build scankey for every attribute in the index. */
	for (attoff = 0; attoff < indnkeyatts; attoff++)
	{
		Oid operator;
		Oid opfamily;
		RegProcedure regop;
		int pkattno = attoff + 1;
		int mainattno = indkey->values[attoff];
		Oid optype = get_opclass_input_type(opclass->values[attoff]);
		/*
		 * Load the operator info.  We need this to get the equality operator
		 * function for the scan key.
		 */
		opfamily = get_opclass_family(opclass->values[attoff]);
		operator= get_opfamily_member(opfamily, optype, optype, BTEqualStrategyNumber);
		regop = get_opcode(operator);

		if (attoff < indnkeyatts - 1)
		{
			/* Initialize segmentby scankeys. */
			if (key_flags & SEGMENTBY_KEYS)
			{
				if (compressedslot)
				{
					ScanKeyInit(&scankey[attoff],
								pkattno,
								BTEqualStrategyNumber,
								regop,
								compressedslot->tts_values[mainattno - 1]);
					if (compressedslot->tts_isnull[mainattno - 1])
						scankey[attoff].sk_flags |= (SK_ISNULL | SK_SEARCHNULL);
					scankey[attoff].sk_collation = idxrel->rd_indcollation[attoff];
					total_keys++;
				}
				else if (uncompressedslot)
				{
					/* get index attribute name */
					Form_pg_attribute attr = TupleDescAttr(idxrel->rd_att, attoff);
					AttrNumber attno = get_attnum(in_rel->rd_id, attr->attname.data);
					ScanKeyInit(&scankey[attoff],
								pkattno,
								BTEqualStrategyNumber,
								regop,
								uncompressedslot->tts_values[attno - 1]);
					if (uncompressedslot->tts_isnull[attno - 1])
						scankey[attoff].sk_flags |= (SK_ISNULL | SK_SEARCHNULL);
					scankey[attoff].sk_collation = idxrel->rd_indcollation[attoff];
					total_keys++;
				}
			}
		}
		else if (attoff == indnkeyatts - 1 && compressedslot)
		{
			/* Initialize _ts_meta_sequence_num scankeys. */
			if (key_flags & NEXT_SEQNUM_KEYS)
			{
				ScanKeyInit(&scankey[attoff],
							pkattno,
							BTEqualStrategyNumber,
							regop,
							compressedslot->tts_values[mainattno - 1] + SEQUENCE_NUM_GAP);
				scankey[attoff].sk_collation = idxrel->rd_indcollation[attoff];
				total_keys++;
			}
		}
	}
	*num_keys = total_keys;
	return scankey;
}

/*
 * Helper method to fetch next compressed tuple based on segmentby and
 * _ts_meta_sequence_num column values from searchslot
 */
static bool
fetch_next_compressed_tuple_using_index(const Relation in_rel, const Oid index_oid,
										const TupleTableSlot *searchslot,
										HeapTuple *compressed_tuple)
{
	Relation idxrel;
	IndexScanDesc index_scan;
	ScanKeyData *scankey = NULL;
	TupleTableSlot *index_slot = NULL;
	int num_scankeys = 0;
	bool ret = false;

	idxrel = index_open(index_oid, AccessShareLock);
	scankey = build_index_scankeys(in_rel,
								   idxrel,
								   searchslot,
								   NULL,
								   &num_scankeys,
								   SEGMENTBY_KEYS | NEXT_SEQNUM_KEYS);
	index_scan = index_beginscan(in_rel, idxrel, GetTransactionSnapshot(), num_scankeys, 0);
	index_rescan(index_scan, scankey, num_scankeys, NULL, 0);
	index_slot = table_slot_create(in_rel, NULL);
	if (index_getnext_slot(index_scan, ForwardScanDirection, index_slot))
	{
		slot_getallattrs(index_slot);
		/* found valid tuple */
		*compressed_tuple = heap_form_tuple(index_slot->tts_tupleDescriptor,
											index_slot->tts_values,
											index_slot->tts_isnull);
		(*compressed_tuple)->t_self = index_slot->tts_tid;
		ret = true;
	}
	else
	{
		compressed_tuple = NULL;
	}
	ExecDropSingleTupleTableSlot(index_slot);
	index_endscan(index_scan);
	index_close(idxrel, AccessShareLock);
	return ret;
}

static bool
fetch_next_compressed_tuple(const RowDecompressor *decompressor,
							const RowCompressor *row_compressor, const Chunk *uncompressed_chunk,
							const int nsegmentby_cols, TupleTableSlot *compressed_slot,
							HeapTuple *next_tuple)
{
	bool tuple_found = false;
	HeapTuple next_compressed_tuple = NULL;
	if (OidIsValid(row_compressor->index_oid))
		tuple_found = fetch_next_compressed_tuple_using_index(decompressor->in_rel,
															  row_compressor->index_oid,
															  compressed_slot,
															  &next_compressed_tuple);
	*next_tuple = next_compressed_tuple;
	return tuple_found;
}

/* Read next uncompressed tuple for Tuplestore */
static bool
read_next_uncompressed_tuple(Tuplesortstate *tuplestore, TupleTableSlot *uncompressed_slot)
{
	return tuplesort_gettupleslot(tuplestore,
								  true /*=forward*/,
								  false /*=copy*/,
								  uncompressed_slot,
								  NULL /*=abbrev*/);
}

/*
 * For a given compressed/uncompressed tuple, check if segmentby/orderby
 * column values matches. This method will also check for nullable
 * segmentby columns values
 */
static void
does_tuples_match(const RowDecompressor *decompressor, HeapTuple compressed_tuple,
				  TupleTableSlot *uncompressed_slot, Bitmapset *null_columns,
				  ScanKeyData *segment_by_keys, int num_segkeys, ScanKeyData *segment_order_by_keys,
				  int num_scankeys, bool *segment_exists, bool *compressed_tuple_found)
{
	if (null_columns)
	{
		/*
		 * we cannot use HeapKeyTest to check for NULLS, thus perform
		 * manual checks by calling has_all_nulls()
		 */
		HeapTuple uncompressed_tuple = ExecCopySlotHeapTuple(uncompressed_slot);
		/* all nullable segmentby columns have matching NULL values */
		bool all_nulls =
			has_all_nulls(decompressor, compressed_tuple, uncompressed_tuple, null_columns);
		heap_freetuple(uncompressed_tuple);
		/* all nullable segmentby columns has NULLS */
		if (all_nulls && num_segkeys == 0)
		{
			*segment_exists = true;
			*compressed_tuple_found = true;
			return;
		}
	}
	if (num_segkeys)
	{
		HeapKeyTest(compressed_tuple,
					RelationGetDescr(decompressor->in_rel),
					num_segkeys,
					segment_by_keys,
					*segment_exists);
		HeapKeyTest(compressed_tuple,
					RelationGetDescr(decompressor->in_rel),
					num_scankeys,
					segment_order_by_keys,
					*compressed_tuple_found);
	}
}

/*
 * In a compressed chunk there can be multiple compressed rows with same segment
 * by values and incrementing sequence numbers. This method will tell if the
 * given tuple (searchslot) is the last tuple with highest sequence number.
 *
 * Return true if searchslot is the last tuple else return false.
 */
static bool
is_compressed_tuple_in_last_batch(const Relation in_rel, const Oid index_oid,
								  const TupleTableSlot *searchslot)
{
	Relation idxrel;
	IndexScanDesc index_scan;
	ScanKeyData *scankey = NULL;
	TupleTableSlot *index_slot;
	int num_scankeys = 0;
	bool ret = false;

	idxrel = index_open(index_oid, AccessShareLock);
	/* Build scankey for every attribute in the index. */
	scankey = build_index_scankeys(in_rel,
								   idxrel,
								   searchslot,
								   NULL,
								   &num_scankeys,
								   SEGMENTBY_KEYS | NEXT_SEQNUM_KEYS);
	index_scan = index_beginscan(in_rel, idxrel, GetTransactionSnapshot(), num_scankeys, 0);
	index_rescan(index_scan, scankey, num_scankeys, NULL, 0);
	index_slot = table_slot_create(in_rel, NULL);
	ret = index_getnext_slot(index_scan, ForwardScanDirection, index_slot);
	ExecDropSingleTupleTableSlot(index_slot);
	index_endscan(index_scan);
	index_close(idxrel, AccessShareLock);
	return !ret;
}

/*
 * Helper method to handle all uncompressed rows which has
 * segmentby columns with new values.
 */
static void
save_new_compressed_rows(const RowDecompressor *decompressor, const Chunk *uncompressed_chunk,
						 Tuplesortstate *uncompressed_rows, AffectedSegmentsCxt **affectedsegments,
						 const int nsegmentby_cols)
{
	CompressedSegmentInfo **current_segment = NULL;
	TupleTableSlot *uncompressed_slot = NULL;
	unsigned int num_non_matching_compressedrows = 0;
	unsigned int new_segment_count = 0;
	uncompressed_slot = MakeTupleTableSlot(decompressor->out_desc, &TTSOpsMinimalTuple);

	while (read_next_uncompressed_tuple(uncompressed_rows, uncompressed_slot))
	{
		slot_getallattrs(uncompressed_slot);
		if (!current_segment)
		{
			/* initialize segment */
			current_segment = palloc(sizeof(CompressedSegmentInfo *) * nsegmentby_cols);
			for (int i = 0; i < nsegmentby_cols; i++)
				current_segment[i] = palloc(sizeof(CompressedSegmentInfo));
			initialize_segment(decompressor,
							   current_segment,
							   uncompressed_chunk,
							   uncompressed_slot);
		}

		/* check if there is change in segmentby value */
		bool is_new_segment =
			is_segment_changed(current_segment, uncompressed_slot, nsegmentby_cols);

		if (is_new_segment)
		{
			/* update new segment */
			update_current_segment(current_segment, uncompressed_slot, nsegmentby_cols);
			if (new_segment_count >= (*affectedsegments)->new_segments_default_size)
				(*affectedsegments) = resize_affectedsegmentscxt((*affectedsegments));
			(*affectedsegments)->total_new_compressed_segments++;
			(*affectedsegments)->num_non_matching_compressedrows[new_segment_count++] =
				num_non_matching_compressedrows;
			num_non_matching_compressedrows = 1;
		}
		else
			num_non_matching_compressedrows++;
	}
	ExecDropSingleTupleTableSlot(uncompressed_slot);
	if (new_segment_count >= (*affectedsegments)->new_segments_default_size)
		(*affectedsegments) = resize_affectedsegmentscxt((*affectedsegments));
	/* last row present in tuplestore should be handled */
	(*affectedsegments)->total_new_compressed_segments++;
	(*affectedsegments)->num_non_matching_compressedrows[new_segment_count++] =
		num_non_matching_compressedrows;
}

/*
 * This method will store all uncompressed rows which match
 * any of the existing segmentby ids in existing_segmentscxt.
 * Uncompressed rows which do not match will be saved in
 * new_segmentscxt.
 */
static void
segregate_uncompressed_rows(const RowDecompressor *decompressor, const Oid index_oid,
							Tuplesortstate *uncompressed_rows,
							CompressTuplesortstateCxt *existing_segmentscxt,
							bool *existing_segments_present,
							CompressTuplesortstateCxt *new_segmentscxt, bool *new_segments_present,
							const int nsegmentby_cols)
{
	Relation index_rel;
	IndexScanDesc index_scan;
	TupleTableSlot *compressed_slot = NULL;
	TupleTableSlot *uncompressed_slot = NULL;
	ScanKeyData *idx_scankey = NULL;
	int num_idx_scankeys = 0;

	compressed_slot = table_slot_create(decompressor->in_rel, NULL);
	uncompressed_slot = MakeTupleTableSlot(decompressor->out_desc, &TTSOpsMinimalTuple);
	/* Index scan */
	index_rel = index_open(index_oid, AccessShareLock);
	index_scan = index_beginscan(decompressor->in_rel,
								 index_rel,
								 GetTransactionSnapshot(),
								 nsegmentby_cols,
								 0);

	while (read_next_uncompressed_tuple(uncompressed_rows, uncompressed_slot))
	{
		slot_getallattrs(uncompressed_slot);
		idx_scankey = build_index_scankeys(decompressor->out_rel,
										   index_rel,
										   NULL,
										   uncompressed_slot,
										   &num_idx_scankeys,
										   SEGMENTBY_KEYS);
		index_rescan(index_scan, idx_scankey, num_idx_scankeys, NULL, 0);

		if (index_getnext_slot(index_scan, ForwardScanDirection, compressed_slot))
		{
			/* segments exists */
			tuplesort_puttupleslot(existing_segmentscxt->tuplestore, uncompressed_slot);
			*existing_segments_present = true;
		}
		else
		{
			tuplesort_puttupleslot(new_segmentscxt->tuplestore, uncompressed_slot);
			*new_segments_present = true;
		}
	}
	index_endscan(index_scan);
	index_close(index_rel, AccessShareLock);
	ExecDropSingleTupleTableSlot(uncompressed_slot);
	ExecDropSingleTupleTableSlot(compressed_slot);
}

/*
 * This method will store a mapping between compressed rows
 * and uncompressed rows.
 *
 * Below 2 members will have the mapping:
 * AffectedSegmentsCxt::compressed_tuples
 * AffectedSegmentsCxt::num_matching_uncompressedrows
 */
static void
save_affected_compressed_rows(const RowDecompressor *decompressor,
							  const RowCompressor *row_compressor, const Chunk *compressed_chunk,
							  const Chunk *uncompressed_chunk, Tuplesortstate *uncompressed_rows,
							  AffectedSegmentsCxt **affectedsegments, const int nsegmentby_cols)
{
	Relation index_rel;
	IndexScanDesc index_scan;
	HeapTuple compressed_tuple = NULL;
	TupleTableSlot *uncompressed_slot = NULL;
	TupleTableSlot *compressed_slot = NULL;
	ScanKeyData *segment_by_keys = NULL;
	ScanKeyData *segment_order_by_keys = NULL;

	int num_segkeys = 0;
	int num_scankeys = 0;
	unsigned int num_uncompressedrows = 0;
	unsigned int uncompressed_row_count = 0;

	bool compressed_tuple_found = false;
	bool segment_exists = false;

	/* Index scan */
	index_rel = index_open(row_compressor->index_oid, AccessShareLock);
	index_scan = index_beginscan(decompressor->in_rel,
								 index_rel,
								 GetTransactionSnapshot(),
								 nsegmentby_cols,
								 0);

	compressed_slot = table_slot_create(decompressor->in_rel, NULL);
	uncompressed_slot = MakeTupleTableSlot(decompressor->out_desc, &TTSOpsMinimalTuple);

	/*
	 * 1. For every uncompressed row do following:
	 *   A. If fetched compressed tuple (CR) from (iii) exists:
	 *       - If uncompressed row matching CR goto step1.
	 *       - If uncompressed row does not match, save total
	 *       - fetched uncompressed row count and CR in AffectedSegmentsCxt
	 *   B. Scan the compressed index
	 *       i. Fetch the compressed tuple based on segmentby values
	 *       ii. If matching segmentby tuple is found check the orderby values
	 *       iii. Save the matching segmentby/orderby compressed row (CR)
	 *       iv. If segmentby values does not match goto stepB
	 *   C. Handle all corner cases as part of steps A,B
	 */
	while (read_next_uncompressed_tuple(uncompressed_rows, uncompressed_slot))
	{
		Bitmapset *null_columns = NULL;
		ScanKeyData *idx_scankey = NULL;
		int num_idx_scankeys = 0;

		num_uncompressedrows++;
		slot_getallattrs(uncompressed_slot);

		if (segment_by_keys)
			pfree(segment_by_keys);
		if (segment_order_by_keys)
			pfree(segment_order_by_keys);
		/* build scan keys to do lookup into uncompressed chunks */
		segment_by_keys = build_segment_order_by_scankeys(decompressor,
														  uncompressed_chunk,
														  uncompressed_slot,
														  &null_columns,
														  &num_segkeys,
														  SEGMENTBY_KEYS);
		segment_order_by_keys = build_segment_order_by_scankeys(decompressor,
																uncompressed_chunk,
																uncompressed_slot,
																NULL,
																&num_scankeys,
																SEGMENTBY_KEYS | ORDERBY_KEYS);
		if (compressed_tuple_found)
		{
			/* reset the flags */
			segment_exists = false;
			compressed_tuple_found = false;
			/* check if current uncompressed tuple matches the compressed tuple */
			does_tuples_match(decompressor,
							  compressed_tuple,
							  uncompressed_slot,
							  null_columns,
							  segment_by_keys,
							  num_segkeys,
							  segment_order_by_keys,
							  num_scankeys,
							  &segment_exists,
							  &compressed_tuple_found);
			/* if there is a match, read next uncompressed tuple */
			if (segment_exists && compressed_tuple_found)
				continue;
			if (!compressed_tuple_found)
			{
				/*
				 * current uncompressed tuple did not fit into previously fetched
				 * compressed tuple.
				 * If current uncompressed tuple belongs to last batch in
				 * current segment, then append these uncompressed tuples, else
				 * new batch gets created assuming that these are new segments.
				 */
				if (!is_compressed_tuple_in_last_batch(decompressor->in_rel,
													   row_compressor->index_oid,
													   compressed_slot) ||
					!segment_exists)
				{
					if (uncompressed_row_count >= (*affectedsegments)->default_size)
						(*affectedsegments) = resize_affectedsegmentscxt((*affectedsegments));
					(*affectedsegments)->num_matching_uncompressedrows[uncompressed_row_count++] =
						num_uncompressedrows - 1;
					num_uncompressedrows = 1;
					(*affectedsegments)
						->compressed_tuples[(*affectedsegments)->total_affected_compressed_rows] =
						heap_copytuple(compressed_tuple);
					heap_freetuple(compressed_tuple);
					(*affectedsegments)->total_affected_compressed_rows++;
				}
				else
				{
					compressed_tuple_found = true;
					continue;
				}
			}
			else
				continue;
		}
		else if (segment_exists)
		{
			/*
			 * In some cases, compressed tuples with current segmentby values exist
			 * however these tuples do not match the orderby values. ex:
			 * uncompressed rows:
			 * ('1999-12-30 15:30:00+05:30'::timestamp with time zone, 1, 2, 3, 22.2, 33.3);
			 * ('1999-12-31 05:30:00+05:30'::timestamp with time zone, 1, 2, 3, 2.2, 3.3);
			 * compressed rows:
			 *  1 |  10 |   1000 | Tue Jan 04 06:36:00 2000 PST | Wed Jan 05 15:54:00 2000 PST
			 *	1 |  20 |   1000 | Sun Jan 02 21:16:00 2000 PST | Tue Jan 04 06:34:00 2000 PST
			 *	1 |  30 |   1000 | Sat Jan 01 11:56:00 2000 PST | Sun Jan 02 21:14:00 2000 PST
			 *	1 |  40 |    598 | Fri Dec 31 16:00:00 1999 PST | Sat Jan 01 11:54:00 2000 PST
			 *
			 * For these kind of situations we need to append these uncompressed rows to
			 * compressed tuple with sequence num = 40
			 */

			HeapKeyTest(compressed_tuple,
						RelationGetDescr(decompressor->in_rel),
						num_segkeys,
						segment_by_keys,
						segment_exists);
			if (!segment_exists)
			{
				if (uncompressed_row_count >= (*affectedsegments)->default_size)
					(*affectedsegments) = resize_affectedsegmentscxt((*affectedsegments));
				(*affectedsegments)->num_matching_uncompressedrows[uncompressed_row_count++] =
					num_uncompressedrows - 1;
				num_uncompressedrows = 1;
				(*affectedsegments)
					->compressed_tuples[(*affectedsegments)->total_affected_compressed_rows] =
					heap_copytuple(compressed_tuple);
				heap_freetuple(compressed_tuple);
				(*affectedsegments)->total_affected_compressed_rows++;
			}
			else
				continue;
		}
		idx_scankey = build_index_scankeys(decompressor->out_rel,
										   index_rel,
										   NULL,
										   uncompressed_slot,
										   &num_idx_scankeys,
										   SEGMENTBY_KEYS);
		index_rescan(index_scan, idx_scankey, num_idx_scankeys, NULL, 0);
		segment_exists = false;
		/*
		 * For every compressed tuple fetched which matches segmentby values:
		 *  - check if uncompressed tuple matches the compressed tuple
		 *  - If orderby matches, save this compressed tuple
		 *  - If only segmentby matches, move to next compressed tuple
		 */
		while (index_getnext_slot(index_scan, ForwardScanDirection, compressed_slot))
		{
			segment_exists = true;
			slot_getallattrs(compressed_slot);
			compressed_tuple = ExecCopySlotHeapTuple(compressed_slot);
			/* reset the flags */
			compressed_tuple_found = false;
			/*
			 * check if current uncompressed tuples orderby columns match the
			 * min/max meta columns from this compressed tuple. If matches set
			 * compressed_tuple_found to true.
			 */
			HeapKeyTest(compressed_tuple,
						RelationGetDescr(decompressor->in_rel),
						num_scankeys,
						segment_order_by_keys,
						compressed_tuple_found);
			/*
			 * Stop index scan for now and for this compressed tuple, find
			 * all matching uncompressed tuples and save this compressed tuple
			 * and total matching uncompressed tuple count in affectedsegments
			 */
			if (compressed_tuple_found)
				break;
		}
	}

	if (uncompressed_row_count >= (*affectedsegments)->default_size)
		(*affectedsegments) = resize_affectedsegmentscxt((*affectedsegments));
	/* last row present in tuplestore should be handled */
	(*affectedsegments)->num_matching_uncompressedrows[uncompressed_row_count++] =
		num_uncompressedrows;
	(*affectedsegments)->compressed_tuples[(*affectedsegments)->total_affected_compressed_rows] =
		heap_copytuple(compressed_tuple);
	heap_freetuple(compressed_tuple);
	(*affectedsegments)->total_affected_compressed_rows++;

	/* cleanup */
	ExecDropSingleTupleTableSlot(uncompressed_slot);
	ExecDropSingleTupleTableSlot(compressed_slot);
	index_endscan(index_scan);
	index_close(index_rel, AccessShareLock);
}

/*
 * Helper method to decompress a compressed tuple.
 * This method will return the sequence number of the current
 * compressed tuple.
 */
static int
decompress_in_tuplestore(RowDecompressor *decompressor, const HeapTuple compressed_tuple,
						 Tuplesortstate *merge_tuplestore)
{
	Assert(HeapTupleIsValid(compressed_tuple));
	heap_deform_tuple(compressed_tuple,
					  decompressor->in_desc,
					  decompressor->compressed_datums,
					  decompressor->compressed_is_nulls);
	/* fetch sequence number of current compressed tuple */
	AttrNumber seq_attnum =
		get_attnum(decompressor->in_rel->rd_id, COMPRESSION_COLUMN_METADATA_SEQUENCE_NUM_NAME);
	/* decompress found tuple and delete from compressed chunk */
	row_decompressor_decompress_row(decompressor, merge_tuplestore);
	TM_FailureData tmfd;
	TM_Result result pg_attribute_unused();
	result = table_tuple_delete(decompressor->in_rel,
								&compressed_tuple->t_self,
								GetCurrentCommandId(true),
								GetTransactionSnapshot(),
								InvalidSnapshot,
								true,
								&tmfd,
								false);
	Assert(result == TM_Ok);
	/* make changes visible */
	CommandCounterIncrement();
	return decompressor->compressed_datums[seq_attnum - 1];
}

static void
recompress_affected_compressed_rows(RowDecompressor *decompressor, RowCompressor *row_compressor,
									AffectedSegmentsCxt *affectedsegments,
									const Chunk *uncompressed_chunk, const int nsegmentby_cols,
									Tuplesortstate *uncompressed_rows,
									const ColumnCompressionInfo **keys, const int n_keys)
{
	TupleTableSlot *compressed_slot = NULL;
	TupleTableSlot *uncompressed_slot = NULL;
	HeapTuple next_tuple = NULL;
	unsigned int total_uncompressed_rows_per_segment = 0;
	unsigned int compressed_row_count = 0;
	Datum metacount_val;
	AttrNumber meta_count;

	compressed_slot = MakeTupleTableSlot(decompressor->in_desc, &TTSOpsHeapTuple);
	while (affectedsegments->total_affected_compressed_rows)
	{
		ExecStoreHeapTuple(affectedsegments->compressed_tuples[compressed_row_count],
						   compressed_slot,
						   false);
		heap_deform_tuple(affectedsegments->compressed_tuples[compressed_row_count],
						  decompressor->in_desc,
						  compressed_slot->tts_values,
						  compressed_slot->tts_isnull);
		meta_count =
			get_attnum(decompressor->in_rel->rd_id, COMPRESSION_COLUMN_METADATA_COUNT_NAME);
		metacount_val = compressed_slot->tts_values[meta_count - 1];

		/*
		 * Calculate number of compressed batches to decompress in case
		 * there is an overflow:
		 * For a given compressed batch with sequence# 30, if there is an overflow,
		 * then we save rest of rows > 1000 in next sequence# which is 31. However
		 * if total_uncompressed_rows_per_segment is > (10 * MAX_ROWS_PER_COMPRESSION)
		 * then we run out of sequence numbers from (31 .. 39) thus we
		 * need to decompress the next segment and repeat the same until we
		 * see that all uncompressed rows fits in.
		 *
		 * (total_uncompressed_rows_per_segment + meta_count) / (10 * MAX_ROWS_PER_COMPRESSION)
		 * should give us, how many following compressed batches to decompress.
		 *
		 * Based on the calculated value, we fetch next compressed batches and perform
		 * recompression.
		 */
		total_uncompressed_rows_per_segment =
			affectedsegments->num_matching_uncompressedrows[compressed_row_count];
		unsigned int total_batches_to_decompress =
			((total_uncompressed_rows_per_segment + metacount_val) /
			 (10 * MAX_ROWS_PER_COMPRESSION));

		CompressTuplesortstateCxt *tuplestorecxt = NULL;
		tuplestorecxt = initialize_tuplestore(decompressor->out_rel, keys, n_keys);
		uncompressed_slot = MakeTupleTableSlot(decompressor->out_desc, &TTSOpsMinimalTuple);
		while (affectedsegments->num_matching_uncompressedrows[compressed_row_count] &&
			   read_next_uncompressed_tuple(uncompressed_rows, uncompressed_slot))
		{
			tuplesort_puttupleslot(tuplestorecxt->tuplestore, uncompressed_slot);
			affectedsegments->num_matching_uncompressedrows[compressed_row_count]--;
		}
		ExecDropSingleTupleTableSlot(uncompressed_slot);
		if (next_tuple)
		{
			/* this compressed tuple might have already been decompressed */
			ItemPointer ptr1 = &(next_tuple)->t_self;
			ItemPointer ptr2 = &affectedsegments->compressed_tuples[compressed_row_count]->t_self;
			if (ItemPointerCompare(ptr1, ptr2) != 0)
			{
				/* decompress affected batch */
				row_compressor->sequence_num =
					decompress_in_tuplestore(decompressor,
											 affectedsegments
												 ->compressed_tuples[compressed_row_count],
											 tuplestorecxt->tuplestore);
			}
			else
			{
				heap_freetuple(next_tuple);
				next_tuple = NULL;
			}
		}
		else
		{
			/* decompress affected batch */
			row_compressor->sequence_num =
				decompress_in_tuplestore(decompressor,
										 affectedsegments->compressed_tuples[compressed_row_count],
										 tuplestorecxt->tuplestore);
		}
		while (total_batches_to_decompress)
		{
			/* fetch next batches which will be affected */
			if (fetch_next_compressed_tuple(decompressor,
											row_compressor,
											uncompressed_chunk,
											nsegmentby_cols,
											compressed_slot,
											&next_tuple))
			{
				/* decompress affected batch */
				decompress_in_tuplestore(decompressor, next_tuple, tuplestorecxt->tuplestore);
			}
			total_batches_to_decompress--;
		}
		tuplesort_performsort(tuplestorecxt->tuplestore);
		recompress_segment(tuplestorecxt->tuplestore, decompressor->out_rel, row_compressor);
		cleanup_tuplestorecxt(tuplestorecxt);
		heap_freetuple(affectedsegments->compressed_tuples[compressed_row_count]);
		compressed_row_count++;
		affectedsegments->total_affected_compressed_rows--;
	}
	ExecDropSingleTupleTableSlot(compressed_slot);
}

static void
do_full_recompression(RowDecompressor *decompressor, RowCompressor *row_compressor,
					  const CompressTuplesortstateCxt *uncompressed_tuplestorecxt)
{
	Relation idxrel;
	IndexScanDesc index_scan;
	TupleTableSlot *compressed_slot = NULL;

	idxrel = index_open(row_compressor->index_oid, AccessShareLock);
	index_scan = index_beginscan(decompressor->in_rel, idxrel, GetTransactionSnapshot(), 0, 0);
	index_rescan(index_scan, NULL, 0, NULL, 0);
	compressed_slot = table_slot_create(decompressor->in_rel, NULL);

	while (index_getnext_slot(index_scan, ForwardScanDirection, compressed_slot))
	{
		slot_getallattrs(compressed_slot);
		HeapTuple tuple = heap_form_tuple(compressed_slot->tts_tupleDescriptor,
										  compressed_slot->tts_values,
										  compressed_slot->tts_isnull);
		tuple->t_self = compressed_slot->tts_tid;
		decompress_in_tuplestore(decompressor, tuple, uncompressed_tuplestorecxt->tuplestore);
		heap_freetuple(tuple);
	}
	ExecDropSingleTupleTableSlot(compressed_slot);
	index_endscan(index_scan);
	index_close(idxrel, AccessShareLock);

	tuplesort_performsort(uncompressed_tuplestorecxt->tuplestore);
	recompress_segment(uncompressed_tuplestorecxt->tuplestore,
					   decompressor->out_rel,
					   row_compressor);
}

static void
recompress_new_compressed_rows(const RowDecompressor *decompressor, RowCompressor *row_compressor,
							   AffectedSegmentsCxt *affectedsegments,
							   Tuplesortstate *uncompressed_rows,
							   const ColumnCompressionInfo **keys, const int n_keys)
{
	TupleTableSlot *uncompressed_slot;
	int count = 0;
	uncompressed_slot = MakeTupleTableSlot(decompressor->out_desc, &TTSOpsMinimalTuple);
	while (affectedsegments->total_new_compressed_segments)
	{
		CompressTuplesortstateCxt *tuplestorecxt =
			initialize_tuplestore(decompressor->out_rel, keys, n_keys);

		while (affectedsegments->num_non_matching_compressedrows[count] &&
			   read_next_uncompressed_tuple(uncompressed_rows, uncompressed_slot))
		{
			tuplesort_puttupleslot(tuplestorecxt->tuplestore, uncompressed_slot);
			affectedsegments->num_non_matching_compressedrows[count]--;
		}
		tuplesort_performsort(tuplestorecxt->tuplestore);
		recompress_segment(tuplestorecxt->tuplestore, decompressor->out_rel, row_compressor);
		cleanup_tuplestorecxt(tuplestorecxt);
		affectedsegments->total_new_compressed_segments--;
		count++;
	}
	ExecDropSingleTupleTableSlot(uncompressed_slot);
}
/*
 * Driver method to perform recompression. This method will take
 * compressed/uncompressed chunk as input which needs to be recompressed.
 * This method expects both compressed/uncompressed chunks have an
 * ExclusiveLock before starting recompression.
 */
void
recompress_partial_chunks(const Chunk *uncompressed_chunk, const Chunk *compressed_chunk)
{
	Relation uncompressed_chunk_rel;
	Relation compressed_chunk_rel;
	RowDecompressor decompressor;
	RowCompressor row_compressor;
	const ColumnCompressionInfo **colinfo_array = NULL;
	const ColumnCompressionInfo **keys = NULL;
	CompressTuplesortstateCxt *uncompressed_tuplestorecxt;
	List *htcols_list;
	ListCell *lc;

	int htcols_listlen = 0;
	int nsegmentby_cols = 0;
	int i = 0;
	unsigned long total_segments_decompressed = 0;
	unsigned long total_new_segments = 0;
	unsigned long total_uncompressed_rows_in_chunk = 0;

	int n_keys = 0;
	int16 *in_column_offsets = NULL;

	htcols_list = ts_hypertable_compression_get(uncompressed_chunk->fd.hypertable_id);
	htcols_listlen = list_length(htcols_list);

	colinfo_array = palloc(sizeof(ColumnCompressionInfo *) * htcols_listlen);
	foreach (lc, htcols_list)
	{
		FormData_hypertable_compression *fd = (FormData_hypertable_compression *) lfirst(lc);
		colinfo_array[i++] = fd;
		if (COMPRESSIONCOL_IS_SEGMENT_BY(fd))
			nsegmentby_cols++;
	}
	in_column_offsets = compress_chunk_populate_keys(uncompressed_chunk->table_id,
													 colinfo_array,
													 htcols_listlen,
													 &n_keys,
													 &keys);

	uncompressed_chunk_rel = RelationIdGetRelation(uncompressed_chunk->table_id);
	compressed_chunk_rel = RelationIdGetRelation(compressed_chunk->table_id);
	decompressor = build_decompressor(compressed_chunk_rel, uncompressed_chunk_rel);
	/* do not need the indexes on the uncompressed chunk as we do not write to it anymore */
	ts_catalog_close_indexes(decompressor.indexstate);

	row_compressor_init(&row_compressor,
						RelationGetDescr(uncompressed_chunk_rel),
						compressed_chunk_rel,
						htcols_listlen,
						colinfo_array,
						in_column_offsets,
						RelationGetDescr(compressed_chunk_rel)->natts,
						true /*need_bistate*/,
						true /*reset_sequence*/,
						RECOMPRESS);

	uncompressed_tuplestorecxt = initialize_tuplestore(uncompressed_chunk_rel, keys, n_keys);
	/* save all rows from uncompressed chunk into tuplestore */
	total_uncompressed_rows_in_chunk =
		save_uncompressed_rows_in_tuplestore(uncompressed_chunk_rel,
											 uncompressed_tuplestorecxt->tuplestore);

	if (total_uncompressed_rows_in_chunk)
	{
		CompressedChunkStats compressedstats = {
			.num_rows_pre_compression = 0,
			.num_rows_post_compression = 0,
			.num_expected_gaps_in_sequences = 0,
			.num_gaps_in_sequences = 0,
			.num_segments = 0,
			.num_batches_per_segments = { 0 },
		};

		if (!OidIsValid(row_compressor.index_oid))
			ereport(ERROR, (errcode(ERRCODE_UNDEFINED_TABLE), errmsg("index does not exist")));

		/* fetch initial statistics of current chunk */
		get_compressed_chunk_statistics(decompressor.in_rel,
										row_compressor.index_oid,
										&compressedstats);
		/*
		 * If there is 30% gap available in compressed chunk, recompress
		 * only affected segments, else call legacy method of recompression
		 * (decompress + compress)
		 * Do full recompression when recompression optimization is disabled
		 */
		if (((compressedstats.num_expected_gaps_in_sequences * 0.3) <=
			 compressedstats.num_gaps_in_sequences) &&
			ts_guc_enable_recompression_optimization)
		{
			bool existing_segments_present = false;
			bool new_segments_present = false;
			CompressTuplesortstateCxt *existing_segmentscxt = NULL;
			CompressTuplesortstateCxt *new_segmentscxt = NULL;

			AffectedSegmentsCxt *affectedsegments =
				(AffectedSegmentsCxt *) palloc0(sizeof(AffectedSegmentsCxt));
			affectedsegments->compressed_tuples =
				palloc0(sizeof(unsigned long) * MAX_ROWS_PER_COMPRESSION);
			affectedsegments->num_matching_uncompressedrows =
				palloc0(sizeof(unsigned long) * MAX_ROWS_PER_COMPRESSION);
			affectedsegments->num_non_matching_compressedrows =
				palloc0(sizeof(unsigned long) * MAX_ROWS_PER_COMPRESSION);
			affectedsegments->default_size = MAX_ROWS_PER_COMPRESSION;
			affectedsegments->new_segments_default_size = MAX_ROWS_PER_COMPRESSION;

			existing_segmentscxt = initialize_tuplestore(uncompressed_chunk_rel, keys, n_keys);
			new_segmentscxt = initialize_tuplestore(uncompressed_chunk_rel, keys, n_keys);
			/* sort the tuplestore */
			tuplesort_performsort(uncompressed_tuplestorecxt->tuplestore);
			segregate_uncompressed_rows(&decompressor,
										row_compressor.index_oid,
										uncompressed_tuplestorecxt->tuplestore,
										existing_segmentscxt,
										&existing_segments_present,
										new_segmentscxt,
										&new_segments_present,
										nsegmentby_cols);
			if (existing_segments_present)
			{
				tuplesort_performsort(existing_segmentscxt->tuplestore);
				save_affected_compressed_rows(&decompressor,
											  &row_compressor,
											  compressed_chunk,
											  uncompressed_chunk,
											  existing_segmentscxt->tuplestore,
											  &affectedsegments,
											  nsegmentby_cols);

				/* mark current scan position to starting point */
				tuplesort_rescan(existing_segmentscxt->tuplestore);
				total_segments_decompressed = affectedsegments->total_affected_compressed_rows;
				if (affectedsegments->total_affected_compressed_rows)
				{
					row_compressor.update_sequence = false;
					recompress_affected_compressed_rows(&decompressor,
														&row_compressor,
														affectedsegments,
														uncompressed_chunk,
														nsegmentby_cols,
														existing_segmentscxt->tuplestore,
														keys,
														n_keys);
				}
			}
			if (new_segments_present)
			{
				/* sort the tuplestore */
				tuplesort_performsort(new_segmentscxt->tuplestore);
				affectedsegments->new_segments_found = true;
				save_new_compressed_rows(&decompressor,
										 uncompressed_chunk,
										 new_segmentscxt->tuplestore,
										 &affectedsegments,
										 nsegmentby_cols);

				total_new_segments = affectedsegments->total_new_compressed_segments;
				if (affectedsegments->total_new_compressed_segments)
				{
					tuplesort_rescan(new_segmentscxt->tuplestore);
					row_compressor.update_sequence = true;
					row_compressor.reset_sequence = true;
					recompress_new_compressed_rows(&decompressor,
												   &row_compressor,
												   affectedsegments,
												   new_segmentscxt->tuplestore,
												   keys,
												   n_keys);
				}
			}
			cleanup_tuplestorecxt(existing_segmentscxt);
			cleanup_tuplestorecxt(new_segmentscxt);
			/* freeup AffectedSegmentsCxt */
			pfree(affectedsegments->compressed_tuples);
			pfree(affectedsegments->num_matching_uncompressedrows);
			pfree(affectedsegments->num_non_matching_compressedrows);
			pfree(affectedsegments);
		}
		else
		{
			total_segments_decompressed = compressedstats.num_rows_post_compression;
			row_compressor.compression_type = COMPRESS;
			do_full_recompression(&decompressor, &row_compressor, uncompressed_tuplestorecxt);
		}
		/****** update compression statistics ******/
		RelationSize after_size = ts_relation_size_impl(compressed_chunk->table_id);
		row_compressor.rowcnt_pre_compression =
			compressedstats.num_rows_pre_compression + total_uncompressed_rows_in_chunk;
		row_compressor.num_compressed_rows =
			compressedstats.num_rows_post_compression + total_new_segments;
		compression_chunk_size_catalog_update_recompressed(uncompressed_chunk->fd.id,
														   compressed_chunk->fd.id,
														   &after_size,
														   row_compressor.rowcnt_pre_compression,
														   row_compressor.num_compressed_rows);
		elog(LOG, "total decompressed segments are: %lu", total_segments_decompressed);
		truncate_relation(uncompressed_chunk_rel->rd_id);
	}
	cleanup_tuplestorecxt(uncompressed_tuplestorecxt);
	row_compressor_finish(&row_compressor);
	FreeBulkInsertState(decompressor.bistate);

	RelationClose(uncompressed_chunk_rel);
	RelationClose(compressed_chunk_rel);
}
