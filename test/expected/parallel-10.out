-- This file and its contents are licensed under the Apache License 2.0.
-- Please see the included NOTICE for copyright information and
-- LICENSE-APACHE for a copy of the license.
--parallel queries require big-ish tables so collect them all here
--so that we need to generate queries only once.
-- look at postgres version to decide whether we run with analyze or without
SELECT
  CASE WHEN current_setting('server_version_num')::int >= 100000
    THEN 'EXPLAIN (analyze, costs off, timing off, summary off)'
    ELSE 'EXPLAIN (costs off)'
  END AS "PREFIX"
\gset
CREATE TABLE test (i int, j double precision, ts timestamp);
SELECT create_hypertable('test','i',chunk_time_interval:=500000);
NOTICE:  adding not-null constraint to column "i"
 create_hypertable 
-------------------
 (1,public,test,t)
(1 row)

--has to be big enough to force at least 2 workers below.
INSERT INTO test SELECT x, x+0.1, _timescaledb_internal.to_timestamp(x*1000)  FROM generate_series(0,1000000-1) AS x;
ANALYZE test;
SET work_mem TO '50MB';
SET force_parallel_mode = 'on';
SET max_parallel_workers_per_gather = 4;
EXPLAIN (costs off) SELECT first(i, j) FROM "test";
                          QUERY PLAN                           
---------------------------------------------------------------
 Finalize Aggregate
   ->  Gather
         Workers Planned: 2
         ->  Partial Aggregate
               ->  Append
                     ->  Parallel Seq Scan on _hyper_1_1_chunk
                     ->  Parallel Seq Scan on _hyper_1_2_chunk
(7 rows)

SELECT first(i, j) FROM "test";
 first 
-------
     0
(1 row)

EXPLAIN (costs off) SELECT last(i, j) FROM "test";
                          QUERY PLAN                           
---------------------------------------------------------------
 Finalize Aggregate
   ->  Gather
         Workers Planned: 2
         ->  Partial Aggregate
               ->  Append
                     ->  Parallel Seq Scan on _hyper_1_1_chunk
                     ->  Parallel Seq Scan on _hyper_1_2_chunk
(7 rows)

SELECT last(i, j) FROM "test";
  last  
--------
 999999
(1 row)

EXPLAIN (costs off) SELECT time_bucket('1 second', ts) sec, last(i, j)
FROM "test"
GROUP BY sec
ORDER BY sec
LIMIT 5;
                                         QUERY PLAN                                         
--------------------------------------------------------------------------------------------
 Limit
   ->  Sort
         Sort Key: (time_bucket('@ 1 sec'::interval, _hyper_1_1_chunk.ts))
         ->  Finalize HashAggregate
               Group Key: (time_bucket('@ 1 sec'::interval, _hyper_1_1_chunk.ts))
               ->  Gather
                     Workers Planned: 2
                     ->  Partial HashAggregate
                           Group Key: time_bucket('@ 1 sec'::interval, _hyper_1_1_chunk.ts)
                           ->  Result
                                 ->  Append
                                       ->  Parallel Seq Scan on _hyper_1_1_chunk
                                       ->  Parallel Seq Scan on _hyper_1_2_chunk
(13 rows)

-- test single copy parallel plan with parallel chunk append
:PREFIX SELECT time_bucket('1 second', ts) sec, last(i, j)
FROM "test"
WHERE length(version()) > 0
GROUP BY sec
ORDER BY sec
LIMIT 5;
                                                     QUERY PLAN                                                     
--------------------------------------------------------------------------------------------------------------------
 Limit (actual rows=5 loops=1)
   ->  Sort (actual rows=5 loops=1)
         Sort Key: (time_bucket('@ 1 sec'::interval, test.ts))
         Sort Method: top-N heapsort 
         ->  Finalize HashAggregate (actual rows=1000 loops=1)
               Group Key: (time_bucket('@ 1 sec'::interval, test.ts))
               ->  Gather (actual rows=1000 loops=1)
                     Workers Planned: 2
                     Workers Launched: 2
                     ->  Partial HashAggregate (actual rows=333 loops=3)
                           Group Key: time_bucket('@ 1 sec'::interval, test.ts)
                           ->  Result (actual rows=333333 loops=3)
                                 One-Time Filter: (length(version()) > 0)
                                 ->  Parallel Custom Scan (ChunkAppend) on test (actual rows=333333 loops=3)
                                       Chunks excluded during startup: 0
                                       ->  Result (actual rows=500000 loops=1)
                                             One-Time Filter: (length(version()) > 0)
                                             ->  Parallel Seq Scan on _hyper_1_1_chunk (actual rows=500000 loops=1)
                                       ->  Result (actual rows=500000 loops=1)
                                             One-Time Filter: (length(version()) > 0)
                                             ->  Parallel Seq Scan on _hyper_1_2_chunk (actual rows=500000 loops=1)
(21 rows)

SELECT time_bucket('1 second', ts) sec, last(i, j)
FROM "test"
GROUP BY sec
ORDER BY sec
LIMIT 5;
           sec            | last 
--------------------------+------
 Wed Dec 31 16:00:00 1969 |  999
 Wed Dec 31 16:00:01 1969 | 1999
 Wed Dec 31 16:00:02 1969 | 2999
 Wed Dec 31 16:00:03 1969 | 3999
 Wed Dec 31 16:00:04 1969 | 4999
(5 rows)

--test variants of histogram
EXPLAIN (costs off) SELECT histogram(i, 1, 1000000, 2) FROM "test";
                          QUERY PLAN                           
---------------------------------------------------------------
 Finalize Aggregate
   ->  Gather
         Workers Planned: 2
         ->  Partial Aggregate
               ->  Append
                     ->  Parallel Seq Scan on _hyper_1_1_chunk
                     ->  Parallel Seq Scan on _hyper_1_2_chunk
(7 rows)

SELECT histogram(i, 1, 1000000, 2) FROM "test";
      histogram      
---------------------
 {1,500000,499999,0}
(1 row)

EXPLAIN (costs off) SELECT histogram(i, 1,1000001,10) FROM "test";
                          QUERY PLAN                           
---------------------------------------------------------------
 Finalize Aggregate
   ->  Gather
         Workers Planned: 2
         ->  Partial Aggregate
               ->  Append
                     ->  Parallel Seq Scan on _hyper_1_1_chunk
                     ->  Parallel Seq Scan on _hyper_1_2_chunk
(7 rows)

SELECT histogram(i, 1, 1000001, 10) FROM "test";
                                 histogram                                  
----------------------------------------------------------------------------
 {1,100000,100000,100000,100000,100000,100000,100000,100000,100000,99999,0}
(1 row)

EXPLAIN (costs off) SELECT histogram(i, 0,100000,5) FROM "test";
                          QUERY PLAN                           
---------------------------------------------------------------
 Finalize Aggregate
   ->  Gather
         Workers Planned: 2
         ->  Partial Aggregate
               ->  Append
                     ->  Parallel Seq Scan on _hyper_1_1_chunk
                     ->  Parallel Seq Scan on _hyper_1_2_chunk
(7 rows)

SELECT histogram(i, 0, 100000, 5) FROM "test";
                histogram                 
------------------------------------------
 {0,20000,20000,20000,20000,20000,900000}
(1 row)

EXPLAIN (costs off) SELECT histogram(i, 10,100000,5) FROM "test";
                          QUERY PLAN                           
---------------------------------------------------------------
 Finalize Aggregate
   ->  Gather
         Workers Planned: 2
         ->  Partial Aggregate
               ->  Append
                     ->  Parallel Seq Scan on _hyper_1_1_chunk
                     ->  Parallel Seq Scan on _hyper_1_2_chunk
(7 rows)

SELECT histogram(i, 10, 100000, 5) FROM "test";
                 histogram                 
-------------------------------------------
 {10,19998,19998,19998,19998,19998,900000}
(1 row)

-- test parallel ChunkAppend
:PREFIX SELECT i FROM "test" WHERE length(version()) > 0;
                                    QUERY PLAN                                     
-----------------------------------------------------------------------------------
 Gather (actual rows=1000000 loops=1)
   Workers Planned: 1
   Workers Launched: 1
   Single Copy: true
   ->  Result (actual rows=1000000 loops=1)
         One-Time Filter: (length(version()) > 0)
         ->  Custom Scan (ChunkAppend) on test (actual rows=1000000 loops=1)
               Chunks excluded during startup: 0
               ->  Result (actual rows=500000 loops=1)
                     One-Time Filter: (length(version()) > 0)
                     ->  Seq Scan on _hyper_1_1_chunk (actual rows=500000 loops=1)
               ->  Result (actual rows=500000 loops=1)
                     One-Time Filter: (length(version()) > 0)
                     ->  Seq Scan on _hyper_1_2_chunk (actual rows=500000 loops=1)
(14 rows)

-- test ChunkAppend with parallel aggregation
:PREFIX SELECT count(*) FROM "test" WHERE length(version()) > 0;
                                               QUERY PLAN                                               
--------------------------------------------------------------------------------------------------------
 Finalize Aggregate (actual rows=1 loops=1)
   ->  Gather (actual rows=3 loops=1)
         Workers Planned: 2
         Workers Launched: 2
         ->  Partial Aggregate (actual rows=1 loops=3)
               ->  Result (actual rows=333333 loops=3)
                     One-Time Filter: (length(version()) > 0)
                     ->  Parallel Custom Scan (ChunkAppend) on test (actual rows=333333 loops=3)
                           Chunks excluded during startup: 0
                           ->  Result (actual rows=500000 loops=1)
                                 One-Time Filter: (length(version()) > 0)
                                 ->  Parallel Seq Scan on _hyper_1_1_chunk (actual rows=500000 loops=1)
                           ->  Result (actual rows=500000 loops=1)
                                 One-Time Filter: (length(version()) > 0)
                                 ->  Parallel Seq Scan on _hyper_1_2_chunk (actual rows=500000 loops=1)
(15 rows)

SELECT count(*) FROM "test" WHERE length(version()) > 0;
  count  
---------
 1000000
(1 row)

-- test ChunkAppend with # workers < # childs
SET max_parallel_workers_per_gather TO 1;
:PREFIX SELECT count(*) FROM "test" WHERE length(version()) > 0;
                                               QUERY PLAN                                               
--------------------------------------------------------------------------------------------------------
 Finalize Aggregate (actual rows=1 loops=1)
   ->  Gather (actual rows=2 loops=1)
         Workers Planned: 1
         Workers Launched: 1
         ->  Partial Aggregate (actual rows=1 loops=2)
               ->  Result (actual rows=500000 loops=2)
                     One-Time Filter: (length(version()) > 0)
                     ->  Parallel Custom Scan (ChunkAppend) on test (actual rows=500000 loops=2)
                           Chunks excluded during startup: 0
                           ->  Result (actual rows=500000 loops=1)
                                 One-Time Filter: (length(version()) > 0)
                                 ->  Parallel Seq Scan on _hyper_1_1_chunk (actual rows=500000 loops=1)
                           ->  Result (actual rows=500000 loops=1)
                                 One-Time Filter: (length(version()) > 0)
                                 ->  Parallel Seq Scan on _hyper_1_2_chunk (actual rows=500000 loops=1)
(15 rows)

SELECT count(*) FROM "test" WHERE length(version()) > 0;
  count  
---------
 1000000
(1 row)

-- test ChunkAppend with # workers > # childs
SET max_parallel_workers_per_gather TO 2;
:PREFIX SELECT count(*) FROM "test" WHERE i >= 500000 AND length(version()) > 0;
                                               QUERY PLAN                                               
--------------------------------------------------------------------------------------------------------
 Finalize Aggregate (actual rows=1 loops=1)
   ->  Gather (actual rows=3 loops=1)
         Workers Planned: 2
         Workers Launched: 2
         ->  Partial Aggregate (actual rows=1 loops=3)
               ->  Result (actual rows=166667 loops=3)
                     One-Time Filter: (length(version()) > 0)
                     ->  Parallel Custom Scan (ChunkAppend) on test (actual rows=166667 loops=3)
                           Chunks excluded during startup: 0
                           ->  Result (actual rows=250000 loops=2)
                                 One-Time Filter: (length(version()) > 0)
                                 ->  Parallel Seq Scan on _hyper_1_2_chunk (actual rows=250000 loops=2)
                                       Filter: (i >= 500000)
(13 rows)

SELECT count(*) FROM "test" WHERE i >= 500000 AND length(version()) > 0;
 count  
--------
 500000
(1 row)

RESET max_parallel_workers_per_gather;
-- now() is not marked parallel safe in PostgreSQL < 12 so using now()
-- in a query will prevent parallelism but CURRENT_TIMESTAMP and
-- transaction_timestamp() are marked parallel safe
:PREFIX SELECT i FROM "test" WHERE ts < CURRENT_TIMESTAMP;
                              QUERY PLAN                               
-----------------------------------------------------------------------
 Gather (actual rows=1000000 loops=1)
   Workers Planned: 1
   Workers Launched: 1
   Single Copy: true
   ->  Custom Scan (ChunkAppend) on test (actual rows=1000000 loops=1)
         Chunks excluded during startup: 0
         ->  Seq Scan on _hyper_1_1_chunk (actual rows=500000 loops=1)
               Filter: (ts < CURRENT_TIMESTAMP)
         ->  Seq Scan on _hyper_1_2_chunk (actual rows=500000 loops=1)
               Filter: (ts < CURRENT_TIMESTAMP)
(10 rows)

:PREFIX SELECT i FROM "test" WHERE ts < transaction_timestamp();
                              QUERY PLAN                               
-----------------------------------------------------------------------
 Gather (actual rows=1000000 loops=1)
   Workers Planned: 1
   Workers Launched: 1
   Single Copy: true
   ->  Custom Scan (ChunkAppend) on test (actual rows=1000000 loops=1)
         Chunks excluded during startup: 0
         ->  Seq Scan on _hyper_1_1_chunk (actual rows=500000 loops=1)
               Filter: (ts < transaction_timestamp())
         ->  Seq Scan on _hyper_1_2_chunk (actual rows=500000 loops=1)
               Filter: (ts < transaction_timestamp())
(10 rows)

-- this won't be parallel query because now() is parallel restricted in PG < 12
:PREFIX SELECT i FROM "test" WHERE ts < now();
                           QUERY PLAN                            
-----------------------------------------------------------------
 Custom Scan (ChunkAppend) on test (actual rows=1000000 loops=1)
   Chunks excluded during startup: 0
   ->  Seq Scan on _hyper_1_1_chunk (actual rows=500000 loops=1)
         Filter: (ts < now())
   ->  Seq Scan on _hyper_1_2_chunk (actual rows=500000 loops=1)
         Filter: (ts < now())
(6 rows)

